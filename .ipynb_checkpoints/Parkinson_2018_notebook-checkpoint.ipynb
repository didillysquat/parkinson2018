{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the 19k ortholog alignment and phylogeny for Parkinson 2018\n",
    "This notebook will document the work that was necessary to go from the following output files:\n",
    "* Filtered assembly .fasta files (one per species)\n",
    "* Predicted ORF files .fasta files (one per species)\n",
    "* Predicted one-to-one ortholog file (one file for all four species)\n",
    "\n",
    "To an amino acid alignment made through the concatenation of all 19000 ortholog peptide sequences and a ML phylogeny.\n",
    "\n",
    "It will also document the dN/dS analysis conducted as part of this study.\n",
    "\n",
    "This documentaion will be split up into three major sections\n",
    "* #### [Fixing multi-ORF prediction](#fixing_multi_orf_prediction)\n",
    "* #### [Creating the super alignment and phylogeny](#creating_the_super_alignment_and_phylogeny)\n",
    "* #### [Creating the guidance alignments (mafft) and dropping low confidence columns](#conducting_alignments_in_guidance)\n",
    "* #### [Creating phylip formated alignment blocks and running CODEML](#making_block_alignments_for_CODEML_analysis)\n",
    "* #### [Creating BUSTED input files and running analyses](#creating_busted_input_files_and_running_analysis)\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fixing_multi_orf_prediction'></a>\n",
    "### Fixing multi-ORF prediction\n",
    "When we were working through the predicted one-to-one ortholog file e.g. /Users/humebc/Google Drive/projects/parky/protein_alignments/a_id.csv we were finding that some of the sets of amino acid sequences (four sequences, one per species) did not align well. Looking back into why this could be we found that the problem lay in the fact that although the post-filtering transcript sequences e.g. compXXX_seqXXX were unique per species assembly file, several ORFs could be predicted per transcript. When the one-to-one data was created, rather than referencing the ORF ID e.g. m.XXX (which is unique per species) the compXXX ID was used. As such, for those transcripts that had several ORFs predicted for them, it was pure chance that the correct ORF (that had been found to be an ortholog of one of the other species ORFs) had been selected.\n",
    "\n",
    "The easiest way to solve this problem would have been to go back to the original input and output of the one-to-one ortholog predictions and work with the unique ORF IDs (e.g. m.XXX). However, this analysis was done several years ago and I was unable to find this file. As such, a different approach was required. To fix the issue I decided to go through each of the ortholog predictions and check the possible combinations of ORFs that could have been selected to find the set of ORFs with the lowest average pairwise distances.\n",
    "\n",
    "E.g. if we consider a single ortholog (ortholog_0), we have the four transcipts identified that the ORFs came from that were predicted to be orthologs, e.g. comp0, comp1, comp2, comp3. So that:\n",
    "```python\n",
    "transect_to_ORF_dict = {\n",
    "    'comp0': ['comp0_m.0', 'comp0_m.1'],\n",
    "    'comp1': ['comp1_m.0'],\n",
    "    'comp2': ['comp2_m.0', 'comp2_m.1'],\n",
    "    'comp3': ['comp3_m.0']\n",
    "}\n",
    "```\n",
    "In this case the possible alignments that could have been selected were:\n",
    "```python\n",
    "list_of_possible_alignements = [\n",
    "    ['comp0_m.0', 'comp1_m.0', 'comp2_m.0', 'comp3_m.0'],\n",
    "    ['comp0_m.0', 'comp1_m.0', 'comp2_m.1', 'comp3_m.0'],\n",
    "    ['comp0_m.1', 'comp1_m.0', 'comp2_m.0', 'comp3_m.0'],\n",
    "    ['comp0_m.1', 'comp1_m.0', 'comp2_m.1', 'comp3_m.0'],   \n",
    "]\n",
    "```\n",
    "For each of these alignments 2 way combinations irregardless of order were permutated and for each of these pairwise comparisons the sequence distance was calculated. For each alignment set, the average pairwise distance was then calculated and the set of sequences with the highest average pairwise alignment was selected as the best set of ORFs to work with.\n",
    "\n",
    "For a sanity check through this process, I regularly inspected the alignments that were being out put as the 'best selections' to make sure that no further errors had occured previously in the one-to-one predictions. All alignments I checked looked great.\n",
    "\n",
    "__Update (23/07/18):__ I am now working towards creating the dN/dS ratios using both CODEML and BUSTED and these will require us to work with the codon-based DNA rather than the AA. As such I will need to make some changes in the code below to make sure that I replace the DNA seqs in the CDS dataframes as well as the AA seqs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''So now we know that there are a few doozies here that we need to take account of.\n",
    "    1 - the comps were not unique and had sequence variations. These have been made unique in the\n",
    "    longest_250 versions of the assemblies and so these are the files we should work with in terms of getting DNA\n",
    "    2 - the comps are not unique across speceis, i.e. each species has a comp00001\n",
    "    3 - after the above longest_250 processing we can essentially assume that comps are unique within species\n",
    "\n",
    "    sooo.... some pseudo code to figure this out\n",
    "    we should work in a dataframe for this\n",
    "    read in the list of ortholog gene IDs for each species (the comps that make up the 19000) (this is the dataframe)\n",
    "    then for each species, identify the gene IDs (comps) for which multiple ORFs were made\n",
    "    go row by row in the dataframe and see if any of the comp IDs (specific to species) are with multiple ORFs\n",
    "    These are our rows of interest, we need to work within these rows:\n",
    "\n",
    "    for each comp for each speceis get a list of the orf aa sequences. turn these into a list of lists\n",
    "    then use itertools.product to get all of the four orfs that could be aligned.\n",
    "    then for each of these possible combinations do itertools.combinations(seqs, 2) and do\n",
    "    pairwise distances and get the average score of the pairwise alignments\n",
    "    we keep the set of four that got the best possible score\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Read in the csv files as pandas dataframes__\n",
    "These will give us the compIDs and the actual aa seqs of the currently predicted ORF variants for each ortholog\n",
    "\n",
    "__Update (23/07/18):__ We will also need to read in the cds dataframe as we will need to mofify this so that we can work with it when doing the dN/dS calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # First get the ID of the transcript that is related to this Ortholog for each species\n",
    "    base_dir = '/home/humebc/projects/parky'\n",
    "    gene_id_df = pd.read_csv('/home/humebc/projects/parky/gene_id.csv', index_col=0)\n",
    "\n",
    "\n",
    "\n",
    "    # We want to be able to compare how many of the alignments we fixed and how many were OK due to luck\n",
    "    # To do this we will need a couple of counters but also the currently chosen ORFs\n",
    "    aa_seq_df = pd.read_csv('/home/humebc/projects/parky/aa_seq.csv', index_col=0)\n",
    "\n",
    "    # we will also need to update the cds_seq_df dataframe\n",
    "    cds_seq_df = pd.read_csv('/home/humebc/projects/parky/cds.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counters for keeping track of how many of the orthologs we had to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_orf_counter = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the files containing which ORFs were predicted for each of the transcripts for each species\n",
    "Hold these in a 2D list, one list per species\n",
    "\n",
    "__Update (23/07/18):__ We will also need to read in the cds version of this file so that we can get the correct DNA sequences and replace these in the cds dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_interleaved_to_sequencial_fasta_two(fasta_in):\n",
    "    fasta_out = []\n",
    "\n",
    "    for i in range(len(fasta_in)):\n",
    "\n",
    "        if fasta_in[i].startswith('>'):\n",
    "            if fasta_out:\n",
    "                # if the fasta is not empty then this is not the first\n",
    "                fasta_out.append(temp_seq_str)\n",
    "            #else then this is the first sequence and there is no need to add the seq.\n",
    "            temp_seq_str = ''\n",
    "            fasta_out.append(fasta_in[i])\n",
    "        else:\n",
    "            temp_seq_str = temp_seq_str + fasta_in[i]\n",
    "    #finally we need to add in the last sequence\n",
    "    fasta_out.append(temp_seq_str)\n",
    "    return fasta_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the predicted orfs\n",
    "# read in the orf aa files for each of the four species\n",
    "try:\n",
    "    comp_to_orfs_dict_holder_list = pickle.load( open('{}/comp_to_orfs_dict_holder_list.pickled'.format(base_dir), 'rb'))\n",
    "    orf_to_aa_dict_holder_list = pickle.load( open('{}/orf_to_aa_dict_holder_list.pickled'.format(base_dir), 'rb'))\n",
    "except:\n",
    "    aa_orf_file_holder_list = []\n",
    "    for spp in list(gene_id_df):\n",
    "        with open('/home/baumgas/projects/done/7_John/assemblies_species/annotation/for-dnds/{}_ORFaa.fa'.format(spp),\n",
    "                  'r') as f:\n",
    "            aa_orf_file_holder_list.append(convert_interleaved_to_sequencial_fasta_two([line.rstrip() for line in f]))\n",
    "\n",
    "    comp_to_orfs_dict_holder_list = [defaultdict(list) for spp in list(gene_id_df)]\n",
    "    orf_to_aa_dict_holder_list = [dict() for spp in list(gene_id_df)]\n",
    "    for i in range(len(list(gene_id_df))):\n",
    "        for j in range(len(aa_orf_file_holder_list[i])):\n",
    "            if aa_orf_file_holder_list[i][j].startswith('>'):\n",
    "                comp_ID = aa_orf_file_holder_list[i][j].split()[8].split('_')[0]\n",
    "                orf_ID = aa_orf_file_holder_list[i][j].split()[0][1:]\n",
    "                comp_to_orfs_dict_holder_list[i][comp_ID].append(orf_ID)\n",
    "                orf_to_aa_dict_holder_list[i][orf_ID] = aa_orf_file_holder_list[i][j + 1]\n",
    "        # print out stats for the default dict to see if we agree with Chris at this point\n",
    "        multi_orf_comps = sum([1 for k, v in comp_to_orfs_dict_holder_list[i].items() if len(v) > 1])\n",
    "        print('{} comps with > 1 ORF in {}'.format(multi_orf_comps, list(gene_id_df)[i]))\n",
    "\n",
    "    pickle.dump( comp_to_orfs_dict_holder_list, open('{}/comp_to_orfs_dict_holder_list.pickled'.format(base_dir), 'wb'))\n",
    "    pickle.dump(orf_to_aa_dict_holder_list, open('{}/orf_to_aa_dict_holder_list.pickled'.format(base_dir), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now do the same for the cds file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will do the same as above but for the codon sequences.\n",
    "# we will create both the com to orf dict and the orf to cds dict.\n",
    "# in theory the comp to orf dict should be eactly the same for the cds file as it was for the aa file\n",
    "# we should check this as a sanity check. If it is the same then we can ignore this second dict we made\n",
    "\n",
    "try:\n",
    "    comp_to_orfs_dict_holder_list_cds = pickle.load( open('{}/comp_to_orfs_dict_holder_list_cds.pickled'.format(base_dir), 'rb'))\n",
    "    orf_to_cds_dict_holder_list = pickle.load( open('{}/orf_to_cds_dict_holder_list.pickled'.format(base_dir), 'rb'))\n",
    "except:\n",
    "    cds_orf_file_holder_list = []\n",
    "    for spp in list(gene_id_df):\n",
    "        with open('/home/baumgas/projects/done/7_John/assemblies_species/annotation/for-dnds/cds/{}_ORFcds.fa'.format(spp),\n",
    "                  'r') as f:\n",
    "            cds_orf_file_holder_list.append(convert_interleaved_to_sequencial_fasta_two([line.rstrip() for line in f]))\n",
    "\n",
    "    comp_to_orfs_dict_holder_list_cds = [defaultdict(list) for spp in list(gene_id_df)]\n",
    "    orf_to_cds_dict_holder_list = [dict() for spp in list(gene_id_df)]\n",
    "    for i in range(len(list(gene_id_df))):\n",
    "        for j in range(len(cds_orf_file_holder_list[i])):\n",
    "            if cds_orf_file_holder_list[i][j].startswith('>'):\n",
    "                comp_ID = cds_orf_file_holder_list[i][j].split()[8].split('_')[0]\n",
    "                orf_ID = cds_orf_file_holder_list[i][j].split()[0][1:]\n",
    "                comp_to_orfs_dict_holder_list_cds[i][comp_ID].append(orf_ID)\n",
    "                orf_to_cds_dict_holder_list[i][orf_ID] = cds_orf_file_holder_list[i][j + 1]\n",
    "        # print out stats for the default dict to see if we agree with Chris at this point\n",
    "        multi_orf_comps = sum([1 for k, v in comp_to_orfs_dict_holder_list_cds[i].items() if len(v) > 1])\n",
    "        print('{} comps with > 1 ORF in {}'.format(multi_orf_comps, list(gene_id_df)[i]))\n",
    "\n",
    "    pickle.dump(comp_to_orfs_dict_holder_list_cds,\n",
    "                open('{}/comp_to_orfs_dict_holder_list_cds.pickled'.format(base_dir), 'wb'))\n",
    "    pickle.dump(orf_to_cds_dict_holder_list, open('{}/orf_to_cds_dict_holder_list.pickled'.format(base_dir), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do sanity check to make sure that the ```compt_to_orfs_dict_holder_list_cds``` is the same as ```compt_to_orfs_dict_holder_list_cds```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK: PASSED!\n",
    "# let's look to make sure that both of the comp_to_orfs dicts are the same from reading in the aa and cds files.\n",
    "for i in range(len(list(gene_id_df))):\n",
    "    comp_dict_aa = comp_to_orfs_dict_holder_list[i]\n",
    "    comp_dict_cds = comp_to_orfs_dict_holder_list_cds[i]\n",
    "    for comp_key in comp_dict_aa.keys():\n",
    "        if set(comp_dict_aa[comp_key]) != set(comp_dict_cds[comp_key]):\n",
    "            sys.exit('The list of ORFs for comp {} do not match between the cds and aa dict'.format(comp_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now make a list from this which is simply the comps that have multiple ORFs predicted for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we have the list of dictionaries from which we can get the comp IDs that have multiple ORFs\n",
    "# perhaps we shoud convert them to lists now rather than on the fly\n",
    "list_of_multi_orf_comps_per_spp = [[k for k, v in comp_to_orfs_dict_holder_list[i].items() if len(v) > 1] for i in range(len(list(gene_id_df)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go through the df of compIDs for each ortholog and for each species look to see if the comp ID is found in the list we just created\n",
    "If it is then this means that there at least one of the speceis has a transcript for which multiple ORFs were predicted. The set of ORFs associated with this ortholog will therefore need to be investigated.\n",
    "\n",
    "__Update (23/07/18):__ Since we will be re-calculating the dN/dS ratios as well we will need to have a codon alignment to work with. We can use the aa s to make the tree but for the CODEML and the BUSTED analyses we would be working with codon alignments. Currently we convert the ORFs to their aa codes without keeping track of the ORF identifier. We then write out a 'fixed' aa dataframe. Instead of doing this we will need to keep track of the ORF ID so that we can also write out the corrected CDS sequence. To do this we have passed a tuple to the MP worker which contains both the aa_seq and the cds_seq. In the end we then fix the cds dataframe as well as the aa dataframe.\n",
    "\n",
    "__Update (26/07/28):__ We were also having some problems with the fact that some of the cds sequences were not %3 compliant. We have incorporated screening into this code so that we drop any ortholog that has one or more ORFs that are not %3 compliant. (approximately 90 were not compliant, but after the alignment fixing here only about 8 were left)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will multiprocess the checks of the problematic orthologs.\n",
    "To do this we will create a list that will hold three items:\n",
    "* A list of tuples. Each tuple containing four sets of both cds and aa sequences. The collection of tuples represent all of the possible alignments that we need to check\n",
    "* The list of cds and aa sequenecs that are currently assigned to the ortholog\n",
    "* The ortholog ID (simply an int)\n",
    "\n",
    "#### create the list that will hold the MP info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  N.B. we have a problem here in that some of the CDS sequences are not 3%0.\n",
    "# On first inspection it appears that 83 of the orthologs have CDS seqs that are not 3%0.\n",
    "# 77 of these happen to be comps that have multi ORFs with 6 not having multi-ORFs\n",
    "# I think it is very dangerous to be dealing with these partial ORFs as we don't know if they are partial in the\n",
    "# 3' end or the 5' end.\n",
    "# As such, I want to get rid of them using the following logic\n",
    "# If one of an orthologs cds seqs is not 3%0 and doesn't have multi ORFs then we just drop this ortholog from\n",
    "# the study.\n",
    "# If above, but there is multi-ORFs then we will first find the best set of ORFs and then check to see if they\n",
    "# contain any non 3%0 cds seqs. If they do then we will drop this ortholog too.\n",
    "\n",
    "col_labels = list(gene_id_df)\n",
    "ortholog_indices_to_drop = []\n",
    "for index in gene_id_df.index.values.tolist():\n",
    "    # within each row check to see if the comp is in its respective list\n",
    "\n",
    "    row_to_check = False\n",
    "    for i in range(len(col_labels)):\n",
    "        if gene_id_df.loc[index, col_labels[i]] in list_of_multi_orf_comps_per_spp[i]:\n",
    "            row_to_check = True\n",
    "    # here we have checked through each of the comps in the row\n",
    "    # if row_to_check == True then we need to work through each of the comps and see which ORF combinations\n",
    "    # produce the best average pairwise distances\n",
    "\n",
    "    if row_to_check:\n",
    "        sys.stdout.write('\\rChecking multi-ORF ortholog {} for MP'.format(index))\n",
    "        multi_orf_counter += 1\n",
    "        list_of_lists_of_possible_orfs = [comp_to_orfs_dict_holder_list[i][gene_id_df.loc[index, col_labels[i]]] for\n",
    "                                          i in\n",
    "                                          range(len(col_labels))]\n",
    "\n",
    "\n",
    "        # convert the orf IDs associated to each of the possible ORFs to both aa_seqs and cds_seqs\n",
    "        list_of_lists_of_possible_orfs_as_aa = [[] for spp in col_labels]\n",
    "        list_of_lists_of_possible_orfs_as_cds = [[] for spp in col_labels]\n",
    "        for k in range(len(list_of_lists_of_possible_orfs)):\n",
    "            for orfID in list_of_lists_of_possible_orfs[k]:\n",
    "                list_of_lists_of_possible_orfs_as_aa[k].append(orf_to_aa_dict_holder_list[k][orfID])\n",
    "                list_of_lists_of_possible_orfs_as_cds[k].append(orf_to_cds_dict_holder_list[k][orfID])\n",
    "\n",
    "\n",
    "        # hardcode it to a list so that we can get the index of each tuple below\n",
    "\n",
    "        # to ensure maintainance of the direct link between the aa seq and the cds_seq it is maybe easiest\n",
    "        # if instead of the unit of the tup being an aa sequence, it is a tuple itself which is (aa_seq, cds_seq)\n",
    "        # this way we can know which of the cds_seqs were chosen based on the AAs/ORFIDs.\n",
    "        # Create this list of list of tuples using the list_of_lists_of_possible_orfs and ..._as_aa lists\n",
    "        list_of_lists_of_aa_to_cds_tups = [[] for spp in col_labels]\n",
    "        for n in range(len(list_of_lists_of_possible_orfs)):\n",
    "            for m in range(len(list_of_lists_of_possible_orfs[n])):\n",
    "                aa_seq = list_of_lists_of_possible_orfs_as_aa[n][m]\n",
    "                cds_seq = list_of_lists_of_possible_orfs_as_cds[n][m]\n",
    "                list_of_lists_of_aa_to_cds_tups[n].append((aa_seq, cds_seq))\n",
    "\n",
    "        # With the new change we should now be passing through a set of tuples that contain the aa_seq and the\n",
    "        # ORF id to the MP\n",
    "        alignment_tuples = [tup for tup in itertools.product(*list_of_lists_of_aa_to_cds_tups)]\n",
    "\n",
    "\n",
    "        mp_list.append((alignment_tuples, aa_seq_df.loc[index].tolist(), cds_seq_df.loc[index].tolist(), index))\n",
    "    else:\n",
    "        # If the row does not need checking for multi-ORFs then we should still check to make sure that\n",
    "        # the cds seqs for this ortholog are modulus 3 compliant else we should get rid of the ORF\n",
    "        drop = False\n",
    "        for spp in col_labels:\n",
    "            if len(cds_seq_df.loc[index, spp])%3 != 0:\n",
    "                drop = True\n",
    "                break\n",
    "        if drop:\n",
    "            ortholog_indices_to_drop.append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the MP worker that will perform the pairwise comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ORF_screening_worker(input_queue, rows_to_be_replaced_dict, indices_to_drop_list):\n",
    "    #also verify that the cds sequences chosen are the same or different\n",
    "    # to do this we will have to pass in the current cds seqs as well.\n",
    "\n",
    "    for tup in iter(input_queue.get, 'STOP'):\n",
    "        alignment_tuples, current_seqs_aa, current_seqs_cds, index = tup\n",
    "        sys.stdout.write('\\rAssessing multi-ORF ortholog: {} for best alignments'.format(index))\n",
    "\n",
    "        # we should now have sets that are four aa sequences each paired with four cds seqs.\n",
    "        # for each set, generate pairwise comparisons and calculate pairwise distances based on the aa_seqs\n",
    "        # keep track of each distance and calculate average PW distances for each set_of_alignemtn_seqs\n",
    "        # also associate which set of ORF_ids produced each average PW distance.\n",
    "        # This list will keep track of this by storing tups that are (average PW distance)\n",
    "        average_distance_list = []\n",
    "        # it is important to note, and maintain the fact that the aa seqs and ORF_ids are in the species order\n",
    "        # inside the alignment_tupes, i.e. first seq is always min\n",
    "        for set_of_alignment_seqs in alignment_tuples:\n",
    "            temp_pairwise_scores_list = []\n",
    "            # here we are comparing two tuples that are (aa_seq, ORF_id)\n",
    "\n",
    "            for seq_a, seq_b in itertools.combinations(set_of_alignment_seqs, 2):\n",
    "\n",
    "                # here is a single PW distance calculation\n",
    "                score = pairwise2.align.globalxx(seq_a[0], seq_b[0], score_only=True)\n",
    "                temp_pairwise_scores_list.append(score)\n",
    "            # now calculate average\n",
    "            temp_average = sum(temp_pairwise_scores_list) / len(temp_pairwise_scores_list)\n",
    "            average_distance_list.append(temp_average)\n",
    "\n",
    "        # now we have a list of PW distance for each of the sets of sequences (virtual alignments)\n",
    "        # we want to select the set that has the highest score\n",
    "\n",
    "        index_of_best_set = average_distance_list.index(max(average_distance_list))\n",
    "        # now check to see if these match those that were already chosen\n",
    "        best_set_of_aa = [tup[0] for tup in alignment_tuples[index_of_best_set]]\n",
    "        best_set_of_cds = [tup[1] for tup in alignment_tuples[index_of_best_set]]\n",
    "        alignments_are_same = True\n",
    "\n",
    "        for i in range(len(current_seqs_aa)):\n",
    "            if current_seqs_aa[i] != best_set_of_aa[i]:\n",
    "                alignments_are_same = False\n",
    "                break\n",
    "\n",
    "        if alignments_are_same:\n",
    "            for i in range(len(current_seqs_cds)):\n",
    "                if current_seqs_cds[i] != best_set_of_cds[i]:\n",
    "                    alignments_are_same = False\n",
    "                    break\n",
    "        for i in range(len(current_seqs_cds)):\n",
    "            if len(best_set_of_cds[i])%3 != 0:\n",
    "                # if any of the new cds seqs break the 3%0 rule then we don't want to drop the ortholog\n",
    "                indices_to_drop_list.append(index)\n",
    "                # set alignments to true so that this ortholog will not be modified in the df s\n",
    "                alignments_are_same = True\n",
    "                break\n",
    "\n",
    "        if not alignments_are_same:\n",
    "\n",
    "            # we don't need to know the actual ORFs that have been chosen for each spp\n",
    "            # only the aa codes so let's just output this result\n",
    "            rows_to_be_replaced_dict[index] = [aa_orf_id_tup for aa_orf_id_tup in alignment_tuples[index_of_best_set]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and run the MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we need to make some changes. The dN/dS analysis that we are going to perform will be based on\n",
    "# codon-based DNA sequences rather than amino acid seuences. As such we will need to have some way of\n",
    "# correcting the cds dataframe as well as the AA dataframe.\n",
    "num_proc = 20\n",
    "\n",
    "#Queue that will hold the index of the rows that need to be checked\n",
    "input_queue = Queue()\n",
    "\n",
    "# populate input_queue\n",
    "for tup in mp_list:\n",
    "    input_queue.put(tup)\n",
    "\n",
    "for i in range(num_proc):\n",
    "    input_queue.put('STOP')\n",
    "\n",
    "# Manager for a dict rows_to_be_replaced_dict that will hold the new (aa_seqs, ORF_id) for the fixed indices\n",
    "manager = Manager()\n",
    "rows_to_be_replaced_dict = manager.dict()\n",
    "list_of_indices_to_drop = manager.list()\n",
    "\n",
    "list_of_processes = []\n",
    "for N in range(num_proc):\n",
    "    p = Process(target=ORF_screening_worker, args=(input_queue, rows_to_be_replaced_dict, list_of_indices_to_drop))\n",
    "    list_of_processes.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in list_of_processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have a dictionary that contains the ID of an ortholog as key and a list of the four aa seqs (and cds_seqs now) (one for each species) that need to be __replaced in the dataframe and then written out__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the multi proc list of indices to drop to the non-multi proc list\n",
    "ortholog_indices_to_drop.extend(list(list_of_indices_to_drop))\n",
    "\n",
    "# print out the orthologs that were non %3 compliant\n",
    "with open('non_mod_3_compliant_orthologs.txt', 'w') as f:\n",
    "    for line in ortholog_indices_to_drop:\n",
    "        f.write('{}\\n'.format(line))\n",
    "\n",
    "# print out the orthologs that needed fixing\n",
    "with open('multi_orf_orthologs_that_needed_fixing.txt', 'w') as f:\n",
    "    for orth_id in rows_to_be_replaced_dict.keys():\n",
    "        f.write('{}\\n'.format(orth_id))\n",
    "\n",
    "# now drop the indices from the dfs (all three)\n",
    "aa_seq_df = aa_seq_df.drop(ortholog_indices_to_drop, axis=0)\n",
    "cds_seq_df = cds_seq_df.drop(ortholog_indices_to_drop, axis=0)\n",
    "gene_id_df = gene_id_df.drop(ortholog_indices_to_drop, axis=0)\n",
    "\n",
    "fixed_counter = len(rows_to_be_replaced_dict.keys())\n",
    "# now replace the dataframe values\n",
    "# we can use a single parse on both the aa and cds\n",
    "for index in aa_seq_df.index.values.tolist():\n",
    "    if index in rows_to_be_replaced_dict.keys():\n",
    "        # then this is a row that needs replcing with the new values\n",
    "        aa_seq_df.loc[index] = [tup[0] for tup in rows_to_be_replaced_dict[index]]\n",
    "        cds_seq_df.loc[index] = [tup[1] for tup in rows_to_be_replaced_dict[index]]\n",
    "\n",
    "\n",
    "# at this point it only remains to write the dataframes out as csv\n",
    "print('{} orthologs were checked due to multi-ORFs\\n'\n",
    "      '{} were fixed\\n'\n",
    "      '{} already contained the optimal choice of ORFs\\n'.format\n",
    "    (\n",
    "    multi_orf_counter, fixed_counter, multi_orf_counter-fixed_counter\n",
    "    )\n",
    ")\n",
    "aa_seq_df.to_csv('/home/humebc/projects/parky/aa_seq_multi_orf_orths__partial_cdsfixed.csv')\n",
    "cds_seq_df.to_csv('/home/humebc/projects/parky/cds_seq_multi_orf_orths__partial_cdsfixed.csv')\n",
    "gene_id_df.to_csv('/home/humebc/projects/parky/gene_id_multi_orf_orths_partial_cds_fixed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='creating_the_super_alignment_and_phylogeny'></a>\n",
    "### Creating the super alignment and phylogeny\n",
    "* Generate local alignment for each ortholog (MAFFT; cropped)\n",
    "* Calculate best fit amino acid substitution matrix (using prottest)\n",
    "* Concatenate the local alignements into super alignment, create q file and make ML tree (using raxml_HPC)\n",
    "\n",
    "__Update (26/07/18):__\n",
    "Since creating this tree I have moved on to re-doing the dN/dS analyses. This has meant putting the alignments through Guidance, to make sure that we are only relying on high confidence parts of the alignments. To run the CODEML and BUSTED components of this analysis I have used the alignments that have had low confidence regions removed. I have not used these slimmed down alignments for the tree making. Using the post-guidance alignments for the tree creation will make next to no difference for the end phylogeny. As such I have not yet reconstructed the tree using the post-guidance alignments.\n",
    "\n",
    "Well aligned regions are of course critical for getting reliable dN/dS scores and as such I feel the guidance analysis is vital."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate local alignment for each ortholog (MAFFT; cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Local functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDefinedFileToList(filename):\n",
    "    temp_list = []\n",
    "    with open(filename, mode='r') as reader:\n",
    "        temp_list = [line.rstrip() for line in reader]\n",
    "    return temp_list\n",
    "\n",
    "def writeListToDestination(destination, listToWrite):\n",
    "    #print('Writing list to ' + destination)\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(destination))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    with open(destination, mode='w') as writer:\n",
    "        i = 0\n",
    "        while i < len(listToWrite):\n",
    "            if i != len(listToWrite)-1:\n",
    "                writer.write(listToWrite[i] + '\\n')\n",
    "            elif i == len(listToWrite)-1:\n",
    "                writer.write(listToWrite[i])\n",
    "            i += 1\n",
    "\n",
    "def convert_interleaved_to_sequencial_fasta_two(fasta_in):\n",
    "    fasta_out = []\n",
    "\n",
    "    for i in range(len(fasta_in)):\n",
    "\n",
    "        if fasta_in[i].startswith('>'):\n",
    "            if fasta_out:\n",
    "                # if the fasta is not empty then this is not the first\n",
    "                fasta_out.append(temp_seq_str)\n",
    "            #else then this is the first sequence and there is no need to add the seq.\n",
    "            temp_seq_str = ''\n",
    "            fasta_out.append(fasta_in[i])\n",
    "        else:\n",
    "            temp_seq_str = temp_seq_str + fasta_in[i]\n",
    "    #finally we need to add in the last sequence\n",
    "    fasta_out.append(temp_seq_str)\n",
    "    return fasta_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the aa and gene ID csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the amino acid sequences\n",
    "aa_seq_array = pd.read_csv('/home/humebc/projects/parky/aa_seq_multi_orf_orths_fixed.csv', sep=',', lineterminator='\\n', index_col=0, header=0)\n",
    "\n",
    "# the gene IDs\n",
    "gene_id_array = pd.read_csv('/home/humebc/projects/parky/gene_id_fixed.csv', sep=',', lineterminator='\\n', index_col=0, header=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the alignments using multiprocessing to speed things up.\n",
    "This worker will perform the alignment and also do the cropping. We will do the cropping by reading in an alignment into a dataframe and then drop the columns from the dataframe that contained gaps from both the beggining and the end, until we come to a column that doesn't contain a gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_local_alignment_worker(input, output_dir, spp_list):\n",
    "    # for each list that represents an ortholog\n",
    "    for k_v_pair in iter(input.get, 'STOP'):\n",
    "\n",
    "        # ortholog_id\n",
    "        ortholog_id = k_v_pair[0]\n",
    "        print('Processing {}'.format(ortholog_id))\n",
    "        # ortholog_spp_seq_list\n",
    "        ortholog_spp_seq_list = k_v_pair[1]\n",
    "\n",
    "        # create the fasta\n",
    "        fasta_file = []\n",
    "\n",
    "        # for each species\n",
    "        # add the name and aa_seq to the fasta_file\n",
    "        for i in range(len(spp_list)):\n",
    "            fasta_file.extend(['>{}_{}'.format(spp_list[i], ortholog_spp_seq_list[i][0]), ortholog_spp_seq_list[i][1]])\n",
    "\n",
    "        # here we have the fasta_file populated\n",
    "\n",
    "        # Write out the new fasta\n",
    "        fasta_output_path = '{}/{}.fasta'.format(output_dir, ortholog_id)\n",
    "        writeListToDestination(fasta_output_path, fasta_file)\n",
    "        # now perform the alignment with MAFFT\n",
    "        mafft = local[\"mafft-linsi\"]\n",
    "        in_file = fasta_output_path\n",
    "        out_file = fasta_output_path.replace('.fasta', '_aligned.fasta')\n",
    "        # now run mafft including the redirect\n",
    "        (mafft[in_file] > out_file)()\n",
    "\n",
    "        # at this point we have the aligned .fasta written to the output directory\n",
    "        # at this point we need to trim the fasta.\n",
    "        # I was going to use trimAl but this doesn't actually have an option to clean up the ends of alignments\n",
    "        # instead, read in the alignment as a TwoD list to a pandas dataframe\n",
    "        # then delete the begining and end columns that contain gap sites\n",
    "        aligned_fasta_interleaved = readDefinedFileToList(out_file)\n",
    "        aligned_fasta = convert_interleaved_to_sequencial_fasta_two(aligned_fasta_interleaved)\n",
    "        array_list = []\n",
    "        for i in range(1, len(aligned_fasta), 2):\n",
    "                array_list.append(list(aligned_fasta[i]))\n",
    "\n",
    "        # make into pandas dataframe\n",
    "        alignment_df = pd.DataFrame(array_list)\n",
    "\n",
    "        # go from either end deleting any columns that have a gap\n",
    "        columns_to_drop = []\n",
    "        for i in list(alignment_df):\n",
    "            # if there is a gap in the column at the beginning\n",
    "            if '-' in list(alignment_df[i]) or '*' in list(alignment_df[i]):\n",
    "                columns_to_drop.append(i)\n",
    "            else:\n",
    "                break\n",
    "        for i in reversed(list(alignment_df)):\n",
    "            # if there is a gap in the column at the end\n",
    "            if '-' in list(alignment_df[i]) or '*' in list(alignment_df[i]):\n",
    "                columns_to_drop.append(i)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # get a list that is the columns indices that we want to keep\n",
    "        col_to_keep = [col_index for col_index in list(alignment_df) if col_index not in columns_to_drop]\n",
    "\n",
    "        # drop the gap columns\n",
    "        alignment_df = alignment_df[col_to_keep]\n",
    "\n",
    "        # here we have the pandas dataframe with the gap columns removed\n",
    "        #convert back to a fasta and write out\n",
    "        cropped_fasta = []\n",
    "        alignment_index_labels = list(alignment_df.index)\n",
    "        for i in range(len(alignment_index_labels)):\n",
    "            seq_name = '>{}_{}'.format(spp_list[i], ortholog_spp_seq_list[i][0])\n",
    "            aa_seq = ''.join(alignment_df.loc[alignment_index_labels[i]])\n",
    "            cropped_fasta.extend([seq_name, aa_seq])\n",
    "\n",
    "        # here we have the cropped and aligned fasta\n",
    "        # write it out\n",
    "        aligned_cropped_fasta_path = fasta_output_path.replace('.fasta', '_aligned_cropped.fasta')\n",
    "        writeListToDestination(aligned_cropped_fasta_path, cropped_fasta)\n",
    "\n",
    "        # here we should be done with the single alignment\n",
    "        print('Local alignment for {} completed'.format(ortholog_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and populate the input Queue for the MP process. Then run.\n",
    "The items in this list will be key value pairs from the dictionary that we will create below. The dictionary will simply be ortholog ID = key and list of aa seqs for that ortholog will be value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making MP data_holder_list\n",
    "tuple_holder_dict = {}\n",
    "#for each ortholog\n",
    "for row_index in aa_seq_array.index.values.tolist():\n",
    "    print('Adding ortholog {} to MP info list'.format(row_index))\n",
    "    ortholog_id_seq_list = []\n",
    "    # for each species.\n",
    "    for spp in list(gene_id_array):\n",
    "        gene_id = gene_id_array[spp][row_index]\n",
    "        aa_seq = aa_seq_array[spp][row_index]\n",
    "        ortholog_id_seq_list.append((gene_id, aa_seq))\n",
    "    tuple_holder_dict[row_index] = ortholog_id_seq_list\n",
    "\n",
    "# creating the MP input queue\n",
    "ortholog_input_queue = Queue()\n",
    "\n",
    "# populate with one key value pair per ortholog\n",
    "for key, value in tuple_holder_dict.items():\n",
    "    print('Placing {} in MP queue'.format(key))\n",
    "    ortholog_input_queue.put((key, value))\n",
    "\n",
    "num_proc = 24\n",
    "\n",
    "# put in the STOPs\n",
    "for N in range(num_proc):\n",
    "    ortholog_input_queue.put('STOP')\n",
    "\n",
    "allProcesses = []\n",
    "\n",
    "# directory to put the local alignments\n",
    "output_dir = '/home/humebc/projects/parky/aa_tree_creation/local_alignments'\n",
    "\n",
    "# the list of species for each ortholog\n",
    "spp_list = [spp for spp in list(gene_id_array)]\n",
    "\n",
    "# Then start the workers\n",
    "for N in range(num_proc):\n",
    "    p = Process(target=create_local_alignment_worker, args=(ortholog_input_queue, output_dir, spp_list))\n",
    "    allProcesses.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in allProcesses:\n",
    "    p.join()\n",
    "\n",
    "# at this point we have the local alignments all written as fasta files to output_dir.\n",
    "# Now it just remains to concatenate and then run the ML tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "### Calculate best fit amino acid substitution matrix (using prottest)\n",
    "We will use prottest to calculate the model for each of the local alignments. We will MP this. This part of the program takes a considerable amount of time and I ran it over night to complete. You can run it using tmux. Once the models had been output I checked to see that none of the outputs had been corupted (and thus would cause problems further down the line) by simply checking the size of the output files. Most files were 33k in size. Files smaller than this were checked (about 20). These all had IO errors and had stopped prematurely. So, I ran the code again (it has a built in check to see if the output has already been produced; in which case it will not re-do). This time the check came up with no files less than 33k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prottest_worker(input_queue):\n",
    "    base_dir = '/home/humebc/projects/parky/aa_tree_creation/local_alignments'\n",
    "    for file_name in iter(input_queue.get, 'STOP'):\n",
    "        input_path = '{}/{}'.format(base_dir, file_name)\n",
    "        output_path = input_path.replace('_aligned_cropped.fasta', '_prottest_result.out')\n",
    "        if os.path.isfile(output_path):\n",
    "            continue\n",
    "        sys.stdout.write('\\rRunning prottest: {}'.format(file_name))\n",
    "        # perform the prottest\n",
    "        prot_path = '/home/humebc/phylogeneticsSoftware/protest/prottest-3.4.2/prottest-3.4.2.jar'\n",
    "        subprocess.run(['java', '-jar', prot_path, '-i', input_path, '-o', output_path, '-all-distributions', '-all']\n",
    "                       , stdout=subprocess.PIPE, stderr=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will find each of the local alignments and run put them into a list which we will MP\n",
    "# for each item we will run prottest with a single thread and output a file\n",
    "# in the concatenate local alignments file we will then create a q file that will\n",
    "# designate the different partitions and which of the substitution models to use.\n",
    "\n",
    "# get a list of all of the fasta names that we will want to concatenate\n",
    "base_dir = '/home/humebc/projects/parky/aa_tree_creation/local_alignments'\n",
    "list_of_files = [f for f in os.listdir(base_dir) if 'aligned_cropped.fasta' in f]\n",
    "\n",
    "\n",
    "num_proc = 12\n",
    "\n",
    "# Queue that will hold the index of the rows that need to be checked\n",
    "input_queue = Queue()\n",
    "\n",
    "# populate input_queue\n",
    "for file_name in list_of_files:\n",
    "    input_queue.put(file_name)\n",
    "\n",
    "for i in range(num_proc):\n",
    "    input_queue.put('STOP')\n",
    "\n",
    "list_of_processes = []\n",
    "for N in range(num_proc):\n",
    "    p = Process(target=prottest_worker, args=(input_queue,))\n",
    "    list_of_processes.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in list_of_processes:\n",
    "    p.join()\n",
    "\n",
    "return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "### Concatenate the local alignements into super alignment, create q file and make ML tree (using raxml_HPC)\n",
    "To do this we read in all of the prot model output files and look to see which model was suggested for each of the local alignments. We make a default dictionary that holds key = model, value = list(the ortholog IDs that use that model). We then go model by model from this dict and within that ortholog ID by ortholog ID to concatenate all of the local alignments together. At the same time that we are doing the concatenation of the local alignments we are also producing the q_file on a model by model basis. The q file is a file that tells raxml how to partition the data and in our case which protein model should be used in conjunction with which partition. By grouping together the local alignments by the model they will use and therefore minimising the partitions we will hopefully be gaining significant advantages in comput time in the raxml stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dictionary of model to ortholog IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The master alignment that we create should be partitioned according to the protein model used.\n",
    "# I have generated all of the .out files which are the outputs from the prottest\n",
    "# We should iter through these and create a dictionary that is a model type as key\n",
    "# and then have a list of the orthologs of that model.\n",
    "# then sort this by the length of the list\n",
    "# then work our way through the local alignments in this order creating the alignment\n",
    "# We will need to generate the p file as we go\n",
    "# this should take the form\n",
    "'''\n",
    "JTT, gene1 = 1-500\n",
    "WAGF, gene2 = 501-800\n",
    "WAG, gene3 = 801-1000\n",
    "\n",
    "'''\n",
    "\n",
    "# get list of the .out prottest files\n",
    "base_dir = '/home/humebc/projects/parky/aa_tree_creation/local_alignments'\n",
    "list_of_prot_out_filenames = [f for f in os.listdir(base_dir) if 'prottest' in f]\n",
    "\n",
    "# iter through the list of protfiles creating the dict relating model to ortholog\n",
    "# we cannnot change the +G or +I for each partition. As such I will define according to the base model\n",
    "model_to_orth_dict = defaultdict(list)\n",
    "for i in range(len(list_of_prot_out_filenames)):\n",
    "    model = ''\n",
    "    file_name = list_of_prot_out_filenames[i]\n",
    "    orth_num = int(file_name.split('_')[0])\n",
    "    with open('{}/{}'.format(base_dir, file_name), 'r') as f:\n",
    "        temp_file_list = [line.rstrip() for line in f]\n",
    "    for j in range(300, len(temp_file_list), 1):\n",
    "        if 'Best model according to BIC' in temp_file_list[j]:\n",
    "            model = temp_file_list[j].split(':')[1].strip().replace('+G','').replace('+I','')\n",
    "            break\n",
    "    if model == '':\n",
    "        sys.exit('Model line not found in {}'.format(orth_num))\n",
    "    model_to_orth_dict[model].append(orth_num)\n",
    "\n",
    "# #N.B. that we cannot have different gamma for different partitions\n",
    "# # Also best advice is not to run +G and +I together.\n",
    "# # As such we only need to extract the base model here i.e. WAG rather than WAG [+G|+I]\n",
    "# for model in model_to_orth_dict\n",
    "\n",
    "print('The 19k sequences are best represented by {} different aa models'.format(len(model_to_orth_dict.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go model by model concatenating and making the q file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we have the dict populated\n",
    "# now sort the dict\n",
    "sorted_model_list = sorted(model_to_orth_dict, key=lambda k: len(model_to_orth_dict[k]), reverse=True)\n",
    "\n",
    "# now go model by model in the sorted_model_list to make the master alignment.\n",
    "\n",
    "# not the most elegant way but I think I'll just create the mast fasta in memory\n",
    "master_fasta = ['>min','', '>pmin', '', '>psyg', '', '>ppsyg', '']\n",
    "\n",
    "# The q file will hold the information for the partitioning of the alignment for the raxml analysis\n",
    "q_file = []\n",
    "for model in sorted_model_list:\n",
    "    q_file_start = len(master_fasta[1]) + 1\n",
    "    sys.stdout.write('\\rProcessing model {} sequences'.format(model))\n",
    "    for orth_num in model_to_orth_dict[model]:\n",
    "        file_name = str(orth_num) + '_aligned_cropped.fasta'\n",
    "        with open('{}/{}'.format(base_dir, file_name), 'r') as f:\n",
    "            temp_list_of_lines = [line.rstrip() for line in f]\n",
    "\n",
    "        for i in range(1, len(temp_list_of_lines), 2):\n",
    "            new_seq = master_fasta[i] + temp_list_of_lines[i]\n",
    "            master_fasta[i] = new_seq\n",
    "    q_file_finish = len(master_fasta[1])\n",
    "    q_file.append('{}, gene{} = {}-{}'.format(\n",
    "        model.upper(), sorted_model_list.index(model) + 1, q_file_start, q_file_finish))\n",
    "\n",
    "# here we have the master fasta and the q file ready to be outputted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write out the master fasta and the q file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now write out the master fasta\n",
    "master_fasta_output_path = '/home/humebc/projects/parky/aa_tree_creation/master.fasta'\n",
    "with open(master_fasta_output_path, 'w') as f:\n",
    "    for line in master_fasta:\n",
    "        f.write('{}\\n'.format(line))\n",
    "\n",
    "# now write out the q file\n",
    "q_file_output_path = '/home/humebc/projects/parky/aa_tree_creation/qfile.q'\n",
    "with open(q_file_output_path, 'w') as f:\n",
    "    for line in q_file:\n",
    "        f.write('{}\\n'.format(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it just remains to run the raxml.\n",
    "There are a lot of options here to get it to run right\n",
    "Breifly (to helpfully save time next time)\n",
    "* -s = input file\n",
    "* -q = the q file that defines the partitions\n",
    "* -x = this switches on rapid bootstrapping and provides a random number to initiate\n",
    "* -f a = Thi means that the summarised tree will also have the bootstrapped values put on it in the output\n",
    "* -p = a seed required by raxml (random number)\n",
    "* -# the number of bootstraps\n",
    "* -n the base of the output files\n",
    "* -w the output directory where the files will be written\n",
    "* -T the threads used (processes)\n",
    "* -m the model used (see comment in code)\n",
    "* -B 0.03 auto determine the number of boostraps required to get stable support values\n",
    "\n",
    "NB I tried to get the AVX2 version of raxml to work but the architechture of Symbiomics did not support it so we are running the AVX. Also please see the comment in the code on the -m flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run raxml\n",
    "#NB note that although we are specificing mdels for each partition, we still need to add the PROTGAMMAIWAG\n",
    "# model argument to the -m flag. This is just a weird operation of raxml and is explained but hidden in the manual\n",
    "# (search for 'If you want to do a partitioned analysis of concatenated'). Raxml will only extract the rate\n",
    "# variation information from this and will ignore the model component e.g. WAG. FYI any model could be used\n",
    "# doesn't have to be WAG.\n",
    "raxml_path = '/home/humebc/phylogeneticsSoftware/raxml/standard-RAxML/raxmlHPC-PTHREADS-AVX'\n",
    "subprocess.run([raxml_path, '-s', master_fasta_output_path, '-q', q_file_output_path,\n",
    "                '-x', '183746', '-f', 'a', '-p', '83746273', '-#', '1000', '-T', '8', '-n', 'parkinson_out',\n",
    "                '-m', 'PROTGAMMAWAG', '-w', '/home/humebc/projects/parky/aa_tree_creation'])\n",
    "\n",
    "print('\\nConstruction of master fasta complete:\\n{}\\n{}'.format(master_fasta_output_path, q_file_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NB the outputs from the raxml are somewhat confusing!__\n",
    "The support values are very hard to find and not displayed correctly using either treegraph2 or figtree. This online viewer did a good job though:\n",
    "http://etetoolkit.org/treeview/?treeid=13316ffd89bd639d52886271ffab262b&algid=\n",
    "The file to look for is the ```RAxML_bipartitionsBranchLabels.parkinson_out``` file. Look in the file and find the [XXX] for the percentage support. In our case 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conducting_alignments_in_guidance'></a>\n",
    "### Creating the guidance alignments (mafft) and dropping low confidence columns\n",
    "In the code below we read in the cds fastas and we put them into guidance where we run a mafft alignment. We then look at the column scores for the aa alignments and from these we drop the columns of the cds alignments that have a score below 0.6. We then crop the alignments. These cds alignments are then what we'll use for the CODEML and the BUSTED analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_local_alignments_for_each_ortholog_two():\n",
    "    # So we were generating an alignment for the aa seqs using mafft and then cropping them\n",
    "    # However, if we are going to use neighbour to do the aligning then that automatically produces us\n",
    "    # a MAFFT alignment and allows us to screen for low scoring site that can additionally be translated\n",
    "    # into removal of uncertain columns.\n",
    "    # similar to how we were creating a directory for each of the aa_seqs we will do the same thing for\n",
    "    # each of the orthologs.\n",
    "\n",
    "    # the cds sequences\n",
    "    cds_seq_df = pd.read_csv('/home/humebc/projects/parky/cds_seq_multi_orf_orths__partial_cdsfixed.csv', sep=',', lineterminator='\\n',\n",
    "                             index_col=0, header=0)\n",
    "\n",
    "    # the list of species for each ortholog\n",
    "    spp_list = [spp for spp in list(cds_seq_df)]\n",
    "\n",
    "\n",
    "    #we will need a directory for each of the orthologs\n",
    "    # in each directory we will have a cds fasta\n",
    "    # to MP this we can simply give a list of the directories and get the ortholog ID from the directory name\n",
    "\n",
    "    # for each ortholog, create directory and write in the fasta of the cds then pass the directory to the MP list\n",
    "\n",
    "    MP_list = []\n",
    "\n",
    "\n",
    "    for row_index in cds_seq_df.index.values.tolist():\n",
    "        temp_fasta = []\n",
    "        sys.stdout.write('\\rGenerating fasta for ortholog {}'.format(row_index))\n",
    "\n",
    "        # populate the fasta\n",
    "        list_of_seq_names = []\n",
    "        for spp in spp_list:\n",
    "            temp_fasta.extend(['>{}_{}'.format(row_index, spp), cds_seq_df.loc[row_index, spp]])\n",
    "            list_of_seq_names.append('{}_{}'.format(row_index, spp))\n",
    "        # write out the fasta into the  directory\n",
    "        base_dir = '/home/humebc/projects/parky/local_alignments/{0}/'.format(row_index)\n",
    "        path_to_fasta = '/home/humebc/projects/parky/local_alignments/{0}/unaligned_cds_{0}.fasta'.format(row_index)\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "        with open(path_to_fasta, 'w') as f:\n",
    "            for line in temp_fasta:\n",
    "                f.write('{}\\n'.format(line))\n",
    "\n",
    "        # add the fasta full path to the MP list\n",
    "        MP_list.append((path_to_fasta, list_of_seq_names))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # creating the MP input queue\n",
    "    ortholog_input_queue = Queue()\n",
    "\n",
    "    # populate with one key value pair per ortholog\n",
    "    for fasta_tup in MP_list:\n",
    "        ortholog_input_queue.put(fasta_tup)\n",
    "\n",
    "    num_proc = 24\n",
    "\n",
    "    # put in the STOPs\n",
    "    for N in range(num_proc):\n",
    "        ortholog_input_queue.put('STOP')\n",
    "\n",
    "    allProcesses = []\n",
    "\n",
    "\n",
    "    # Then start the workers\n",
    "    for N in range(num_proc):\n",
    "        p = Process(target=guidance_worker, args=(ortholog_input_queue,))\n",
    "        allProcesses.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for p in allProcesses:\n",
    "        p.join()\n",
    "\n",
    "        # here we have the cds and the aa alignments cropped and written out\n",
    "        # we can then use these as input into CODEML and the BUSTED programs\n",
    "        # and to run the model calls for making the tree with.\n",
    "\n",
    "def guidance_worker(input_queue):\n",
    "    for fasta_tup in iter(input_queue.get, 'STOP'):\n",
    "        fasta_file_full_path, seq_names = fasta_tup\n",
    "        ortholog_id = fasta_file_full_path.split('/')[-2]\n",
    "        sys.stdout.write('\\rPerforming Guidance for {}'.format(ortholog_id))\n",
    "        guidance_full_path = '/home/humebc/phylogeneticsSoftware/guidance2/guidance.v2.02/www/Guidance/guidance.pl'\n",
    "        output_dir_full_path = '/'.join(fasta_file_full_path.split('/')[:-1])\n",
    "\n",
    "        # check to see if guidance has already been completed\n",
    "\n",
    "        # if os.path.isfile('{}/{}.cropped_aligned_cds.fasta'.format(output_dir_full_path, ortholog_id)):\n",
    "        #     continue\n",
    "\n",
    "        # subprocess.run(['perl', guidance_full_path, '--seqFile',\n",
    "        #                 fasta_file_full_path, '--msaProgram', 'MAFFT', '--seqType',\n",
    "        #                 'codon', '--outDir', output_dir_full_path, '--bootstraps', '10', '--outOrder', 'as_input',\n",
    "        #                 '--colCutoff', '0.6', '--dataset', ortholog_id], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "        # # at this point we should have performed the guidance analysis\n",
    "        # # we can now read in the cols that should be removed from the cds and aa alignments\n",
    "        # cols_to_remove_cds = []\n",
    "        # cds_cols_to_remove_file_path = 'Seqs.Orig_DNA.fas.FIXED.{}.MAFFT.Removed_Col'.format(ortholog_id)\n",
    "        # with open(cds_cols_to_remove_file_path, 'r') as f:\n",
    "        #     cds_col_file_list = [line.rstrip() for line in f]\n",
    "        # for line in cds_col_file_list:\n",
    "        #     cols_to_remove_cds.append(int(line.split()[2]))\n",
    "\n",
    "        # at this point we should have performed the guidance analysis\n",
    "        # we can now read in the scores for each aa residue and drop any alignment columns\n",
    "        # that have only -na scores or contain a score < the cutoff which is 0.6\n",
    "        # we will be working in sets of four because there are four sequences. Each column\n",
    "        # we find to delte in this aa score matrix will represent three columns to be dropped\n",
    "        # in the cds alignment.\n",
    "        # also, bear in mind that the columns of the alignment are not 0 indexed but start at 1\n",
    "        # we will have to take this into account when working out which columns of the alignments to drop\n",
    "\n",
    "        cols_to_remove_aa = []\n",
    "        aa_cols_score_file_path = '{}/{}.MAFFT.Guidance2_res_pair_res.PROT.scr'.format(output_dir_full_path, ortholog_id)\n",
    "\n",
    "        with open(aa_cols_score_file_path, 'r') as f:\n",
    "            aa_col_score_file_list = [line.rstrip() for line in f][1:]\n",
    "\n",
    "        for i in range(int(len(aa_col_score_file_list)/4)):\n",
    "\n",
    "            socres_set = []\n",
    "            # get a list of the scores for each position\n",
    "            # for j in range(i, i+4, 1):\n",
    "            #     scores_set.append()\n",
    "            # the range that represents i and the next three i s in the iteration without increasing i\n",
    "            file_line_indices = [i*4 + k for k in range(4)]\n",
    "            scores_set = [float(aa_col_score_file_list[j].split()[2]) if '-nan' not in aa_col_score_file_list[j] else '-nan' for j in file_line_indices]\n",
    "\n",
    "            # now examine what is in the score sets\n",
    "\n",
    "            drop = False\n",
    "            nan_count = 0\n",
    "            for score in scores_set:\n",
    "                if score != '-nan':\n",
    "                    if score < 0.6:\n",
    "                        drop = True\n",
    "                        break\n",
    "                elif score == '-nan':\n",
    "                    nan_count += 1\n",
    "                else:\n",
    "                    continue\n",
    "            # when we come out ned to check if the nan_score == 4\n",
    "            if nan_count == 4:\n",
    "                drop = True\n",
    "\n",
    "\n",
    "            # if drop = True then this is a column to drop\n",
    "            if drop:\n",
    "                cols_to_remove_aa.append(i)\n",
    "\n",
    "        # here we have a 0 based list of the columns that need to be dropped from the aa alignment\n",
    "        # convert this to a 0 based index of the columns that need to be dropped from the cds alignment\n",
    "        cols_to_remove_cds = []\n",
    "        for col_index in cols_to_remove_aa:\n",
    "            cols_to_remove_cds.extend([col_index*3 + n for n in range(3)])\n",
    "\n",
    "        # here we have the indices that need to be dropped for the cds and aa alignments\n",
    "        # now read in the alignments as 2D lists, convert to pandas dataframe and perform the columns drops\n",
    "\n",
    "        # aa first\n",
    "        aa_alignment_file_path = '{}/{}.MAFFT.PROT.aln'.format(output_dir_full_path, ortholog_id)\n",
    "        with open(aa_alignment_file_path, 'r') as f:\n",
    "            aa_alignment_file_list = convert_interleaved_to_sequencial_fasta_two([line.rstrip() for line in f])\n",
    "        aa_df = pd.DataFrame([list(line) for line in aa_alignment_file_list if not line.startswith('>')])\n",
    "\n",
    "        # now drop the columns\n",
    "        columns_to_keep_aa = [col for col in list(aa_df) if col not in cols_to_remove_aa]\n",
    "        aa_df = aa_df[columns_to_keep_aa]\n",
    "\n",
    "        # cds second\n",
    "        cds_alignment_file_path = '{}/{}.MAFFT.aln'.format(output_dir_full_path, ortholog_id)\n",
    "        with open(cds_alignment_file_path, 'r') as f:\n",
    "            cds_alignment_file_list = convert_interleaved_to_sequencial_fasta_two([line.rstrip() for line in f])\n",
    "        cds_df = pd.DataFrame([list(line) for line in cds_alignment_file_list if not line.startswith('>')])\n",
    "\n",
    "        # now drop the columns\n",
    "        columns_to_keep_cds = [col for col in list(cds_df) if col not in cols_to_remove_cds]\n",
    "        cds_df = cds_df[columns_to_keep_cds]\n",
    "\n",
    "        # here we have the cds and aa dfs that we can now do the cropping with and then finally write back out as\n",
    "        # fasta files\n",
    "        # go from either end deleting any columns that have a gap\n",
    "\n",
    "        # aa first\n",
    "        aa_df = crop_fasta_df(aligned_fasta_as_pandas_df_to_crop = aa_df)\n",
    "\n",
    "        # cds second\n",
    "        cds_df = crop_fasta_df(aligned_fasta_as_pandas_df_to_crop = cds_df)\n",
    "\n",
    "        # now we just need to write out dfs\n",
    "        aa_fasta_list = pandas_df_to_fasta(pd_df=aa_df, seq_names=seq_names)\n",
    "        with open('{}/{}.cropped_aligned_aa.fasta'.format(output_dir_full_path, ortholog_id), 'w') as f:\n",
    "            for line in aa_fasta_list:\n",
    "                f.write('{}\\n'.format(line))\n",
    "\n",
    "        cds_fasta_list = pandas_df_to_fasta(pd_df=cds_df, seq_names=seq_names)\n",
    "        with open('{}/{}.cropped_aligned_cds.fasta'.format(output_dir_full_path, ortholog_id), 'w') as f:\n",
    "            for line in cds_fasta_list:\n",
    "                f.write('{}\\n'.format(line))\n",
    "\n",
    "        # here we have the cds and the aa alignments cropped and written out\n",
    "        # we can then use these as input into CODEML and the BUSTED programs\n",
    "\n",
    "def pandas_df_to_fasta(pd_df, seq_names):\n",
    "    temp_fasta = []\n",
    "    for i in range(len(seq_names)):\n",
    "        temp_fasta.extend(['>{}'.format(seq_names[i]), ''.join(list(pd_df.loc[i]))])\n",
    "    return temp_fasta\n",
    "\n",
    "\n",
    "def crop_fasta_df(aligned_fasta_as_pandas_df_to_crop):\n",
    "    columns_to_drop = []\n",
    "    for i in list(aligned_fasta_as_pandas_df_to_crop):\n",
    "        # if there is a gap in the column at the beginning\n",
    "        if '-' in list(aligned_fasta_as_pandas_df_to_crop[i]) or '*' in list(aligned_fasta_as_pandas_df_to_crop[i]):\n",
    "            columns_to_drop.append(i)\n",
    "        else:\n",
    "            break\n",
    "    for i in reversed(list(aligned_fasta_as_pandas_df_to_crop)):\n",
    "        # if there is a gap in the column at the end\n",
    "        if '-' in list(aligned_fasta_as_pandas_df_to_crop[i]) or '*' in list(aligned_fasta_as_pandas_df_to_crop[i]):\n",
    "            columns_to_drop.append(i)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # get a list that is the columns indices that we want to keep\n",
    "    col_to_keep = [col_index for col_index in list(aligned_fasta_as_pandas_df_to_crop) if col_index not in columns_to_drop]\n",
    "    # drop the gap columns\n",
    "    return aligned_fasta_as_pandas_df_to_crop[col_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='making_block_alignments_for_CODEML_analysis'></a>\n",
    "### Creating phylip formated alignment blocks and running CODEML\n",
    "The below code takes the guidance-created cds alignments and creates phylip formatted block alignments for the CODEML analysis. So the [documentation](http://nebc.nerc.ac.uk/bioinformatics/documentation/paml/doc/pamlDOC.pdf) for the CODEML input and analysis parameters are not so clear. Essentially, there are four ways to run the CODEML analysis, site specific, branch specific, site-branch specific and then there is what we want which is no site and no branch so just one dN/dS score per gene comparison. CODEML works by calling the execuatble (from its home directory; it relies on a relatively linked file) and by providing a control file that specifies which alignment to work with, which tree to work with, and where to output. It also specifies what sort of model we want to work with. For doing the pair-based analysis of the genes between species we can set the run mode to -2 (this is buried in the [docs](http://nebc.nerc.ac.uk/bioinformatics/documentation/paml/doc/pamlDOC.pdf)). You will see that we create the control file in the code below. You can run CODEML with a single gene at a time but a much faster way to do it is to provide it a phylip formated alignment file that contains multiple alignments (simply one alignment after the other). You can then tell CODEML in the control file how many alignments it should be expecting. Becuase there is no ability to MP analyses directly using the CODEML executable we split the ~19K genes into 20 blocks. Each block is an alignment file with 1000 alignemtns in it (except for the last one obviously which contained the remainder). We can then spawn a given number of processess that set off a CODEML analysis providing each of the blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create the block alignments__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_block_phylip_alignments_for_CODEML():\n",
    "    ''' The documentation for what format the files should be in for submitting to PAML/CODEML are not so\n",
    "    great. But from some testing and looking through the examples that are available two things seem to be key\n",
    "    1 - automatic pairwise comparisons of sequences can be performed using the runmode as -2\n",
    "    2 - a blocked alignment format is the easiest way to test all of the orthologs at once but unlike\n",
    "    in the documentation, (using the G marker) this is easiest done by simply having a phymil alignment format\n",
    "    one after another in succestion in a sinlge file and then setting the ndata setting to how ever many\n",
    "    sets of aligments you have (about 19K for us). I've run some tests and this all seems to be running well.\n",
    "    Unfortunately, the only way to MP this seems to be physically splitting up the data and running multiple\n",
    "    instance of the CODEML executable.\n",
    "    So... with this in mind, I will split up the 19K or so sequences into phylip alignments of 1000 sequences.\n",
    "    I will then make respective control files for each of these, put them in their own directory and then set\n",
    "    an instance of the CODEML running for each of these.'''\n",
    "\n",
    "\n",
    "    spp_list = ['min', 'pmin', 'psyg', 'ppsyg']\n",
    "    # for each pairwise comparison of the species\n",
    "    wkd = '/home/humebc/projects/parky/local_alignments'\n",
    "    output_dir = '/home/humebc/projects/parky/guidance_analyses'\n",
    "    list_of_guidance_dirs = []\n",
    "    # set that will hold the ID of any orthologs that have dud alignments\n",
    "    len_zero_dir_list = []\n",
    "    # counter that we'll use to split into 20 roughly 1k sequence alignments\n",
    "    counter = 0\n",
    "    block_counter = 0\n",
    "\n",
    "    # for each directory or ortholog\n",
    "    list_of_dirs = list()\n",
    "    for root, dirs, files in os.walk(wkd):\n",
    "        list_of_dirs = dirs\n",
    "        break\n",
    "    for dir in list_of_dirs:\n",
    "\n",
    "        sys.stdout.write('\\rOrtholog: {}     {}/{}'.format(dir, counter, len(list_of_dirs)))\n",
    "\n",
    "        # make a new alignment file for every 1000 individual alignments\n",
    "        if counter % 1000 == 0:\n",
    "            if block_counter != 0:\n",
    "\n",
    "                # then we already have a block that needs writing\n",
    "                seq_file_ctrl_file_tup = write_out_cntrl_and_seq_file(block_counter=block_counter, output_dir=output_dir,\n",
    "                                             phylip_alignment=phylip_alignment, num_align=1000)\n",
    "                list_of_guidance_dirs.append(seq_file_ctrl_file_tup)\n",
    "            # once the old block is written out start a new one\n",
    "            block_counter += 1\n",
    "            os.makedirs('{}/block_{}'.format(output_dir, block_counter), exist_ok=True)\n",
    "            print('\\n\\nStarting block {}'.format(block_counter))\n",
    "            phylip_alignment = []\n",
    "\n",
    "        # add single to block master alignment\n",
    "        # if the fasta was empty then this will return False\n",
    "        single_phylip = generate_phylip_from_fasta(dir, wkd)\n",
    "\n",
    "        if single_phylip:\n",
    "            phylip_alignment.extend(single_phylip)\n",
    "            counter += 1\n",
    "        else:\n",
    "            # if the fasta was empty then log this and don't add anything to the counter\n",
    "            len_zero_dir_list.append(dir)\n",
    "    # write out a list of the poorl alignement orfs\n",
    "    with open('post_guidance_0_len_cds_alignments_orthologs.txt', 'w') as f:\n",
    "        for line in len_zero_dir_list:\n",
    "            f.write('{}\\n'.format(line))\n",
    "\n",
    "    # now write out the final block of alignments\n",
    "    seq_file_ctrl_file_tup = write_out_cntrl_and_seq_file(block_counter, output_dir, phylip_alignment, num_align=len(list_of_dirs)-len(len_zero_dir_list)-(1000*(block_counter-1)))\n",
    "    list_of_guidance_dirs.append(seq_file_ctrl_file_tup)\n",
    "\n",
    "    pickle.dump(list_of_guidance_dirs, open( '{}/list_of_guidance_dirs.pickle'.format(output_dir), \"wb\" ))\n",
    "\n",
    "def generate_phylip_from_fasta(dir, wkd):\n",
    "    temp_str = str()\n",
    "    temp_list = list()\n",
    "\n",
    "    with open('{}/{}/{}.cropped_aligned_cds.fasta'.format(wkd, dir, dir), 'r') as f:\n",
    "        fasta_file = [line.rstrip() for line in f]\n",
    "\n",
    "    if len(fasta_file[1]) == 0:\n",
    "        # if the fasta is empty then we need to log this outside\n",
    "        return False\n",
    "    else:\n",
    "        for line in fasta_file:\n",
    "            if line.startswith('>'):\n",
    "                temp_str = line[1:]\n",
    "            else:\n",
    "                temp_list.append('{}    {}'.format(temp_str, line))\n",
    "        # finally put in the header file\n",
    "        temp_list.insert(0, '\\t{} {}'.format(len(temp_list), len(fasta_file[1])))\n",
    "\n",
    "        return temp_list\n",
    "\n",
    "\n",
    "def write_out_cntrl_and_seq_file(block_counter, output_dir, phylip_alignment, num_align):\n",
    "    # write out the control file specific to this alignment\n",
    "    ctrl_file_path = write_out_control_file(\n",
    "        output_dir='{}/block_{}'.format(output_dir, block_counter),\n",
    "        num_alignments=num_align,\n",
    "        grp=block_counter)\n",
    "    # write out the phylip file\n",
    "    seq_file_path = '{}/block_{}/block_{}_cds.phylip'.format(output_dir, block_counter, block_counter)\n",
    "    with open(seq_file_path, 'w') as f:\n",
    "        for line in phylip_alignment:\n",
    "            f.write('{}\\n'.format(line))\n",
    "    # write out the tree file\n",
    "    tree_file = '(ppsyg:0.01524804457090833502,(min:0.00305561548082329418,pmin:0.00350296114601793013)' \\\n",
    "                ':0.03350192310501232812,psyg:0.01618135662493049715);'\n",
    "    tree_file_path = '{}/block_{}/block_{}_tree.nwk'.format(output_dir, block_counter, block_counter)\n",
    "    with open(tree_file_path, 'w') as f:\n",
    "        f.write('{}\\n'.format(tree_file))\n",
    "\n",
    "    return (seq_file_path, ctrl_file_path, tree_file_path)\n",
    "\n",
    "def write_out_control_file(output_dir, num_alignments, grp):\n",
    "    seq_file_path = '{}/block_{}_cds.phylip'.format(output_dir, grp)\n",
    "    out_file_path = '{}/block_{}_guidance_results.out'.format(output_dir, grp)\n",
    "    ctrl_path     = '{}/block_{}_cds.ctrl'.format(output_dir, grp)\n",
    "    tree_file_path = '{}/block_{}/block_{}_tree.nwk'.format(output_dir, grp, grp)\n",
    "    ctrl_file = [\n",
    "    'seqfile = {}'.format(seq_file_path),\n",
    "    'treefile = {}'.format(tree_file_path),\n",
    "    'outfile = {}'.format(out_file_path),\n",
    "    'runmode = -2',\n",
    "    'seqtype = 1',\n",
    "    'CodonFreq = 2',\n",
    "    'ndata = {}'.format(num_alignments),\n",
    "    'clock = 0',\n",
    "    'model = 0',\n",
    "    'NSsites = 0',\n",
    "    'icode = 0',\n",
    "    'fix_omega = 0',\n",
    "    'omega = .4',\n",
    "    'cleandata = 0'\n",
    "    ]\n",
    "\n",
    "    with open(ctrl_path, 'w') as f:\n",
    "        for line in ctrl_file:\n",
    "            f.write('{}\\n'.format(line))\n",
    "\n",
    "    return ctrl_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the CODEML analyses__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CODEML_analyses():\n",
    "    ''' We should read in the tuples that contain the seq files and cntrl file\n",
    "    from the pickled files that were written out from generate_master_phlip_alignments_for_CODEML\n",
    "    We should then start an MP list with each of these tuples in\n",
    "    In the worker we should go to each directory and start an anlysis\n",
    "    The 20000 sequences were broken into 20 chunks so we should start 20 subprocess instances and\n",
    "    have a one CODEML analysis run in each'''\n",
    "\n",
    "    tup_of_dirs_list = pickle.load( open( '/home/humebc/projects/parky/guidance_analyses/list_of_guidance_dirs.pickle', \"rb\" ))\n",
    "\n",
    "    input_queue = Queue()\n",
    "\n",
    "    for tup in tup_of_dirs_list:\n",
    "        input_queue.put(tup)\n",
    "\n",
    "    num_proc = 1\n",
    "\n",
    "    for i in range(num_proc):\n",
    "        input_queue.put('STOP')\n",
    "\n",
    "    list_of_processes = []\n",
    "    for N in range(num_proc):\n",
    "        p = Process(target=CODEML_run_worker, args=(input_queue,))\n",
    "        list_of_processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for p in list_of_processes:\n",
    "        p.join()\n",
    "\n",
    "\n",
    "def CODEML_run_worker(input_queue):\n",
    "    CODEML_path = '/home/humebc/phylogeneticsSoftware/paml/paml4.9h/bin/codeml'\n",
    "    for dir_tup in iter(input_queue.get, 'STOP'):\n",
    "        seq_file_path, ctrl_file_path, tree_file_path = dir_tup\n",
    "\n",
    "        wkd = os.path.dirname(seq_file_path)\n",
    "\n",
    "        os.chdir(wkd)\n",
    "\n",
    "        subprocess.run([CODEML_path, ctrl_file_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Collect the results of the CODEML analysis into one dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_CODEML_pairwise_analyses():\n",
    "    ''' Here we will go through each of the block analyses and take out the pairwise dn/ds analyses and put\n",
    "    them into a single pandas dataframe. The ortholog will the index (which we will evenutally sort) and the\n",
    "    columns will be each of the pairwise comparisons.'''\n",
    "\n",
    "    block_analysis_base_dir = '/home/humebc/projects/parky/guidance_analyses'\n",
    "\n",
    "    list_of_dirs = list()\n",
    "    for root, dirs, files in os.walk(block_analysis_base_dir):\n",
    "        list_of_dirs = dirs\n",
    "        break\n",
    "\n",
    "    # we will create a index system for the columns so that each pairwise difference will be in a specific column\n",
    "    # of the df that we are going to create\n",
    "    # col1 = min_pmin\n",
    "    # col2 = min_psyg\n",
    "    # col3 = min_ppsyg\n",
    "    # col4 = pmin_psyg\n",
    "    # col5 = pmin_ppsyg\n",
    "    # col6 = psyg_ppsyg\n",
    "\n",
    "    comparisons = ['min_pmin', 'min_psyg', 'min_ppsyg', 'pmin_psyg', 'pmin_ppsyg', 'psyg_ppsyg']\n",
    "    df = pd.DataFrame(columns=comparisons)\n",
    "    count = 0\n",
    "    # the list that we will hold the info for a single row in\n",
    "    dict_of_dnns_values = {}\n",
    "    for block_dir in list_of_dirs:\n",
    "        sys.stdout.write('\\n\\nProcessing block {}\\n\\n'.format(block_dir))\n",
    "        # read in the file for this block and populate the df with the dN/dS videos\n",
    "        with open('{0}/{1}/{1}_guidance_results.out'.format(block_analysis_base_dir, block_dir), 'r') as f:\n",
    "            out_file = [line.rstrip() for line in f]\n",
    "\n",
    "        # go line by line through the output file picking up the dnds values and putting them into the df\n",
    "        for i in range(len(out_file)):\n",
    "            if 'Data set' in out_file[i]:\n",
    "                sys.stdout.write('\\rProcessing {}'.format(out_file[i]))\n",
    "            # see if the line in question contains dnds values\n",
    "            if 'dN/dS=' in out_file[i]:\n",
    "                # check to see if we have populated a row worth of dnds values\n",
    "                # if so, populate the df and then start a new row list to collect values in\n",
    "                if count % 6 == 0 and count != 0:\n",
    "                    # then we should have a set of dnds values that we can append to the df\n",
    "                    df.loc[int(ortholog_id)] = [dict_of_dnns_values[pair] for pair in comparisons]\n",
    "                    dict_of_dnns_values = {}\n",
    "\n",
    "                # when we are here we are either adding one more value to an already exisiting set\n",
    "                # or we are starting a brand new set\n",
    "\n",
    "                # get the dn/ds value and the pair that we are working with\n",
    "                dn_ds_value = float(out_file[i].split()[7])\n",
    "                orth_and_pair_info_line = out_file[i-4]\n",
    "                pair_one = orth_and_pair_info_line.split()[1]\n",
    "                pair_two = orth_and_pair_info_line.split()[4]\n",
    "                ortholog_id = pair_one.split('_')[0][1:]\n",
    "                spp_one = pair_one.split('_')[1][:-1]\n",
    "                spp_two = pair_two.split('_')[1][:-1]\n",
    "\n",
    "                if '{}_{}'.format(spp_one, spp_two) in comparisons:\n",
    "                    dict_of_dnns_values['{}_{}'.format(spp_one, spp_two)] = dn_ds_value\n",
    "                else:\n",
    "                    dict_of_dnns_values['{}_{}'.format(spp_two, spp_one)] = dn_ds_value\n",
    "\n",
    "                count += 1\n",
    "    # finally add the last set of dn/ds data to the df\n",
    "    df.loc[int(ortholog_id)] = [dict_of_dnns_values[pair] for pair in comparisons]\n",
    "    pickle.dump( df, open('{}/CODEML_dnds_pairwise_pandas_df.pickle'.format(block_analysis_base_dir), 'wb'))\n",
    "\n",
    "    apples = 'asdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create a stats output for the CODEML analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_stats_for_CODEML_analysis():\n",
    "    block_analysis_base_dir = '/home/humebc/projects/parky/guidance_analyses'\n",
    "\n",
    "    df = pickle.load( open('{}/CODEML_dnds_pairwise_pandas_df.pickle'.format(block_analysis_base_dir), 'rb'))\n",
    "    df.sort_index(inplace=True)\n",
    "    df.to_csv('/home/humebc/projects/parky/guidance_analyses/CODEML_dN_dS.csv')\n",
    "    comparisons = ['min_pmin', 'min_psyg', 'min_ppsyg', 'pmin_psyg', 'pmin_ppsyg', 'psyg_ppsyg']\n",
    "\n",
    "    \n",
    "    count_dict = defaultdict(list)\n",
    "\n",
    "    \n",
    "    pairwise_dict = defaultdict(list)\n",
    "\n",
    "    total_count = 0\n",
    "    for index in df.index.values.tolist():\n",
    "        sys.stdout.write('\\rProcessing index: {}'.format(index))\n",
    "        count = 0\n",
    "        for comp in comparisons:\n",
    "            if df.loc[index, comp] > 1 and df.loc[index, comp] != float(99):\n",
    "                count +=1\n",
    "                total_count += 1\n",
    "                pairwise_dict[comp].append(index)\n",
    "        count_dict[count].append(index)\n",
    "\n",
    "    f = open('{}/CODEML_dN_dS_results.txt'.format(block_analysis_base_dir), 'w')\n",
    "\n",
    "    print('PAIRWISE COUNTS:')\n",
    "    f.write('PAIRWISE COUNTS:\\n')\n",
    "    for comp in comparisons:\n",
    "        print('{}: {}'.format(comp, len(pairwise_dict[comp])))\n",
    "        f.write('{}: {}\\n'.format(comp, len(pairwise_dict[comp])))\n",
    "    print('\\n\\nCOUNTS')\n",
    "    f.write('\\n\\nCOUNTS\\n')\n",
    "    count = 0\n",
    "    for i in [1,2,3,4, 5, 6]:\n",
    "        print('Orthologs with {} ORF dN/dS > 1: {} '.format(i, len(count_dict[i])))\n",
    "        f.write('Orthologs with {} ORF dN/dS > 1: {} \\n'.format(i, len(count_dict[i])))\n",
    "        count += len(count_dict[i])\n",
    "    print('\\n\\nNumber of orthologs with at least one >1 dN/dS scoring ORF: {}'.format(count))\n",
    "    f.write('\\n\\nNumber of orthologs with at least one >1 dN/dS scoring ORF: {}\\n'.format(count))\n",
    "    f.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='creating_busted_input_files_and_running_analysis'></a>\n",
    "### Creating BUSTED input files and running analysis\n",
    "BUSTED analyses can be run through an online GUI called datamonkey. Or it can be run on the command line. However, running it on the command line is annoying because it is interactive. As such, to be able to run it from python on all of the 19K genes (one analysis per gene) we had to create 'answer_files' that contained the answers to the interactive questions that the busted executable wanted to run. You will see that these are generated in the code below. To do this we called the BUSTED executable with subprocess.run but we modified the input argument passing in the actual answers and also the stdout so that it was piped straight into an output file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create a directory per gene and put the tree and fasta in. Then run analysis MP__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories_and_run_busted():\n",
    "    ''' The BUSTED annoyingly has an interactive interface. It does accept special batch files, but I really\n",
    "    don't want to have to invest the time in learning how to use those just for this analysis.\n",
    "    Instead I will simply use a text file that had the predetermined answers to the interactive prompts in\n",
    "    order to start analyses.\n",
    "    To conduct the analyses we will have to go ortholog by ortholog, pulling out the fasta alignment\n",
    "    and then writing this to a new directory were we will also write the tree. Then we simply write\n",
    "    the answer file and run the command.\n",
    "\n",
    "    We should definitely MP this. To do this we should read in the directories\n",
    "    and put these in as the MP list arguments. We should then make all of the files etc and run the analyses\n",
    "    within these threads.\n",
    "\n",
    "    we need to remember to check that alignments aren't empty.'''\n",
    "\n",
    "    local_dirs_dir = '/home/humebc/projects/parky/local_alignments'\n",
    "    list_of_dirs = list()\n",
    "    for root, dirs, files in os.walk(local_dirs_dir):\n",
    "        list_of_dirs = dirs\n",
    "        break\n",
    "\n",
    "    input_queue = Queue()\n",
    "\n",
    "    for dir in list_of_dirs:\n",
    "        input_queue.put(dir)\n",
    "\n",
    "    num_proc = 40\n",
    "\n",
    "    for i in range(num_proc):\n",
    "        input_queue.put('STOP')\n",
    "\n",
    "    list_of_processes = []\n",
    "    for N in range(num_proc):\n",
    "        p = Process(target=BUSTED_MP_worker, args=(input_queue,))\n",
    "        list_of_processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for p in list_of_processes:\n",
    "        p.join()\n",
    "\n",
    "    apples = 'asdf'\n",
    "\n",
    "def BUSTED_MP_worker(input_queue):\n",
    "    busted_analyses_dir = '/home/humebc/projects/parky/busted_analyses'\n",
    "    local_dirs_dir = '/home/humebc/projects/parky/local_alignments'\n",
    "\n",
    "\n",
    "    # The HYPHYMP executable relies on some sort of relative file but I can't find out which one\n",
    "    # as such we will have to change dir to the HYPHY_idr in order to be able to invoke the program.\n",
    "    HYPHYMP_path = '/home/humebc/phylogeneticsSoftware/hyphy/hyphy-2.3.13/HYPHYMP'\n",
    "    HYPHY_dir = '/home/humebc/phylogeneticsSoftware/hyphy/hyphy-2.3.13'\n",
    "    for ortholog_id in iter(input_queue.get, 'STOP'):\n",
    "\n",
    "        # First check to see if the busted processing has already been done for this ortholog\n",
    "        # if it has then skip onto the next one.\n",
    "        if os.path.isfile('{0}/{1}/{1}_qced_cds_aligned.fasta.BUSTED.json'.format(busted_analyses_dir, ortholog_id)):\n",
    "            continue\n",
    "\n",
    "        # First check to see that the alignment is good\n",
    "        orig_align_path = '{0}/{1}/{1}.cropped_aligned_cds.fasta'.format(local_dirs_dir, ortholog_id)\n",
    "        with open(orig_align_path, 'r') as f:\n",
    "            orig_fasta = [line.rstrip() for line in f]\n",
    "\n",
    "        if len(orig_fasta[1]) < 2:\n",
    "            # then the fasta is bad and we should move onto the next directory and ignore this one.\n",
    "            continue\n",
    "\n",
    "\n",
    "        sys.stdout.write('\\rRunning BUSTED analysis on ortholog: {}'.format(ortholog_id))\n",
    "        tree_file = '({0}_ppsyg:0.01524804457090833502,({0}_min:0.00305561548082329418,{0}_pmin:0.00350296114601793013)' \\\n",
    "                    ':0.03350192310501232812,{0}_psyg:0.01618135662493049715);'.format(ortholog_id)\n",
    "\n",
    "        wkd = '{}/{}'.format(busted_analyses_dir, ortholog_id)\n",
    "        os.makedirs(wkd, exist_ok=True)\n",
    "\n",
    "        # write out the tree to the busted analysis directory in question\n",
    "        tree_path = '{}/{}_tree.nwk'.format(wkd, ortholog_id)\n",
    "        with open(tree_path, 'w') as f:\n",
    "            f.write('{}\\n'.format(tree_file))\n",
    "\n",
    "        # copy the cds fasta alignment from the local directory to this directory\n",
    "\n",
    "        new_align_path = '{}/{}_qced_cds_aligned.fasta'.format(wkd, ortholog_id)\n",
    "        shutil.copyfile(orig_align_path, new_align_path)\n",
    "\n",
    "        # here we now have the alignment and the tree file in the directory we want.\n",
    "\n",
    "        # now write out the text file that we will use to put in our answeres to the interactive prompts\n",
    "        answers_script = ['1', '5', '1', new_align_path, tree_path, '1']\n",
    "        answers_script_path = '{}/{}_answers'.format(wkd, ortholog_id)\n",
    "        with open(answers_script_path, 'w') as f:\n",
    "            for line in answers_script:\n",
    "                f.write('{}\\n'.format(line))\n",
    "\n",
    "        # change dir to the hyphy dir\n",
    "        output_file_path = '{}/{}_results.out'.format(wkd, ortholog_id)\n",
    "        os.chdir(HYPHY_dir)\n",
    "        # HYPHYMP_cmd = './HYPHYMP < {} > {}'.format(answers_script_path, output_file_path)\n",
    "        # p = subprocess.Popen(HYPHYMP_cmd, shell=True)\n",
    "        # os.waitpid(p.pid, 0)\n",
    "        # subprocess.run(['./HYPHYMP', '<', answers_script_path, '>',  output_file_path])\n",
    "        with open(output_file_path, 'w') as output:\n",
    "            subprocess.run([HYPHYMP_path], stdout=output, input='\\n'.join(answers_script) + '\\n', encoding='ascii')\n",
    "\n",
    "        apples = 'adsf'\n",
    "\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Collect the busted analyses into a dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_BUSTED_analyses():\n",
    "    busted_base_dir = '/home/humebc/projects/parky/busted_analyses'\n",
    "\n",
    "    list_of_dirs = list()\n",
    "    for root, dirs, files in os.walk(busted_base_dir):\n",
    "        list_of_dirs = dirs\n",
    "        break\n",
    "\n",
    "    manager = Manager()\n",
    "    managed_list = manager.list()\n",
    "\n",
    "    input_queue = Queue()\n",
    "\n",
    "    for dir in list_of_dirs:\n",
    "        input_queue.put(dir)\n",
    "\n",
    "    num_proc = 40\n",
    "\n",
    "    for i in range(num_proc):\n",
    "        input_queue.put('STOP')\n",
    "\n",
    "    list_of_processes = []\n",
    "    for N in range(num_proc):\n",
    "        p = Process(target=collect_busted_MP_worker, args=(input_queue,managed_list))\n",
    "        list_of_processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for p in list_of_processes:\n",
    "        p.join()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.DataFrame([tup[1] for tup in list(managed_list)], columns=['p_value_div_selct'], index=[tup[0] for tup in list(managed_list)])\n",
    "    # for ortholog_id, p_value_float in list(managed_list):\n",
    "    #     df.loc[int(ortholog_id)] = p_value_float\n",
    "    df.sort_index(inplace=True)\n",
    "    pickle.dump(df, open('{}/BUSTED_p_value_results.pickle'.format(busted_base_dir), 'wb'))\n",
    "    df.to_csv('{}/BUSTED_p_value_results.csv'.format(busted_base_dir))\n",
    "    apples = 'asdf'\n",
    "\n",
    "def collect_busted_MP_worker(input_queue, managed_list):\n",
    "    busted_base_dir = '/home/humebc/projects/parky/busted_analyses'\n",
    "    for directory in iter(input_queue.get, 'STOP'):\n",
    "        sys.stdout.write('\\rCollecting ortholog for {}'.format(directory))\n",
    "        with open('{0}/{1}/{1}_results.out'.format(busted_base_dir, directory)) as f:\n",
    "            output_file = [line.rstrip() for line in f]\n",
    "\n",
    "        for i in range(len(output_file)):\n",
    "            if output_file[i] == 'MATRIX':\n",
    "                ortholog_id = int(output_file[i + 1].split('_')[0].lstrip()[1:])\n",
    "            if 'Likelihood ratio test for episodic' in output_file[i]:\n",
    "                p_value_float = float(output_file[i].split()[-1][:-3])\n",
    "\n",
    "        managed_list.append((ortholog_id, p_value_float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Out put a stats file from the df__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_busted_results_stats():\n",
    "    busted_base_dir = '/home/humebc/projects/parky/busted_analyses'\n",
    "    df = pickle.load( open('{}/BUSTED_p_value_results.pickle'.format(busted_base_dir), 'rb'))\n",
    "\n",
    "    sig_at_dict = defaultdict(list)\n",
    "\n",
    "    for index in df.index.values.tolist():\n",
    "        p_val = df.loc[index, 'p_value_div_selct']\n",
    "        if p_val < 0.05:\n",
    "            sig_at_dict[0.05].append(index)\n",
    "        if p_val < 0.01:\n",
    "            sig_at_dict[0.01].append(index)\n",
    "        if p_val < 0.001:\n",
    "            sig_at_dict[0.001].append(index)\n",
    "\n",
    "    f = open('{}/summary_stats_for_busted_p_vals.txt'.format(busted_base_dir), 'w')\n",
    "\n",
    "    print('Number of orthologs found to have significant diversifying selection at:\\n\\n')\n",
    "    print('p < 0.05 = {}'.format(len(sig_at_dict[0.05])))\n",
    "    print('p < 0.01 = {}'.format(len(sig_at_dict[0.01])))\n",
    "    print('p < 0.001 = {}'.format(len(sig_at_dict[0.001])))\n",
    "    f.write('Number of orthologs found to have significant diversifying selection at:\\n\\n')\n",
    "    f.write('p < 0.05 = {}\\n'.format(len(sig_at_dict[0.05])))\n",
    "    f.write('p < 0.01 = {}\\n'.format(len(sig_at_dict[0.01])))\n",
    "    f.write('p < 0.001 = {}\\n'.format(len(sig_at_dict[0.001])))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
