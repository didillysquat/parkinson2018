{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the 19k ortholog alignment and phylogeny for Parkinson 2018\n",
    "This notebook will document the work that was necessary to go from the following output files:\n",
    "* Filtered assembly .fasta files (one per species)\n",
    "* Predicted ORF files .fasta files (one per species)\n",
    "* Predicted one-to-one ortholog file (one file for all four species)\n",
    "\n",
    "To an amino acid alignment made through the concatenation of all 19000 ortholog peptide sequences and a ML phylogeny.\n",
    "\n",
    "This documentaion will be split up into two major sections\n",
    "#### * Fixing multi-ORF prediction\n",
    "#### * Creating the super alignment and phylogeny\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing multi-ORF prediction\n",
    "When we were working through the predicted one-to-one ortholog file e.g. /Users/humebc/Google Drive/projects/parky/protein_alignments/a_id.csv we were finding that some of the sets of amino acid sequences (four sequences, one per species) did not align well. Looking back into why this could be we found that the problem lay in the fact that although the post-filtering transcript sequences e.g. compXXX_seqXXX were unique per species assembly file, several ORFs could be predicted per transcript. When the one-to-one data was created, rather than referencing the ORF ID e.g. m.XXX (which is unique per species) the compXXX ID was used. As such, for those transcripts that had several ORFs predicted for them, it was pure chance that the correct ORF (that had been found to be an ortholog of one of the other species ORFs) had been selected.\n",
    "\n",
    "The easiest way to solve this problem would have been to go back to the original input and output of the one-to-one ortholog predictions and work with the unique ORF IDs (e.g. m.XXX). However, this analysis was done several years ago and I was unable to find this file. As such, a different approach was required. To fix the issue I decided to go through each of the ortholog predictions and check the possible combinations of ORFs that could have been selected to find the set of ORFs with the lowest average pairwise distances.\n",
    "\n",
    "E.g. if we consider a single ortholog (ortholog_0), we have the four transcipts identified that the ORFs came from that were predicted to be orthologs, e.g. comp0, comp1, comp2, comp3. So that:\n",
    "```python\n",
    "transect_to_ORF_dict = {\n",
    "    'comp0': ['comp0_m.0', 'comp0_m.1'],\n",
    "    'comp1': ['comp1_m.0'],\n",
    "    'comp2': ['comp2_m.0', 'comp2_m.1'],\n",
    "    'comp3': ['comp3_m.0']\n",
    "}\n",
    "```\n",
    "In this case the possible alignments that could have been selected were:\n",
    "```python\n",
    "list_of_possible_alignements = [\n",
    "    ['comp0_m.0', 'comp1_m.0', 'comp2_m.0', 'comp3_m.0'],\n",
    "    ['comp0_m.0', 'comp1_m.0', 'comp2_m.1', 'comp3_m.0'],\n",
    "    ['comp0_m.1', 'comp1_m.0', 'comp2_m.0', 'comp3_m.0'],\n",
    "    ['comp0_m.1', 'comp1_m.0', 'comp2_m.1', 'comp3_m.0'],   \n",
    "]\n",
    "```\n",
    "For each of these alignments 2 way combinations irregardless of order were permutated and for each of these pairwise comparisons the sequence distance was calculated. For each alignment set, the average pairwise distance was then calculated and the set of sequences with the highest average pairwise alignment was selected as the best set of ORFs to work with.\n",
    "\n",
    "For a sanity check through this process, I regularly inspected the alignments that were being out put as the 'best selections' to make sure that no further errors had occured previously in the one-to-one predictions. All alignments I checked looked great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''So now we know that there are a few doozies here that we need to take account of.\n",
    "    1 - the comps were not unique and had sequence variations. These have been made unique in the\n",
    "    longest_250 versions of the assemblies and so these are the files we should work with in terms of getting DNA\n",
    "    2 - the comps are not unique across speceis, i.e. each species has a comp00001\n",
    "    3 - after the above longest_250 processing we can essentially assume that comps are unique within species\n",
    "\n",
    "    sooo.... some pseudo code to figure this out\n",
    "    we should work in a dataframe for this\n",
    "    read in the list of ortholog gene IDs for each species (the comps that make up the 19000) (this is the dataframe)\n",
    "    then for each species, identify the gene IDs (comps) for which multiple ORFs were made\n",
    "    go row by row in the dataframe and see if any of the comp IDs (specific to species) are with multiple ORFs\n",
    "    These are our rows of interest, we need to work within these rows:\n",
    "\n",
    "    for each comp for each speceis get a list of the orf aa sequences. turn these into a list of lists\n",
    "    then use itertools.product to get all of the four orfs that could be aligned.\n",
    "    then for each of these possible combinations do itertools.combinations(seqs, 2) and do\n",
    "    pairwise distances and get the average score of the pairwise alignments\n",
    "    we keep the set of four that got the best possible score\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Read in the csv files as pandas dataframes__\n",
    "These will give us the compIDs and the actual aa seqs of the currently predicted ORF variants for each ortholog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the ID of the transcript that is related to this Ortholog for each species\n",
    "gene_id_df = pd.read_csv('/home/humebc/projects/parky/gene_id.csv', index_col=0)\n",
    "\n",
    "# We want to be able to compare how many of the alignments we fixed and how many were OK due to luck\n",
    "# To do this we will need a couple of counters but also the currently chosen ORFs\n",
    "aa_seq_df = pd.read_csv('/home/humebc/projects/parky/aa_seq_fixed_again.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counters for keeping track of how many of the orthologs we had to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_orf_counter = 0\n",
    "fixed_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the files containing which ORFs were predicted for each of the transcripts for each species\n",
    "Hold these in a 2D list, one list per species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_interleaved_to_sequencial_fasta_two(fasta_in):\n",
    "    fasta_out = []\n",
    "\n",
    "    for i in range(len(fasta_in)):\n",
    "\n",
    "        if fasta_in[i].startswith('>'):\n",
    "            if fasta_out:\n",
    "                # if the fasta is not empty then this is not the first\n",
    "                fasta_out.append(temp_seq_str)\n",
    "            #else then this is the first sequence and there is no need to add the seq.\n",
    "            temp_seq_str = ''\n",
    "            fasta_out.append(fasta_in[i])\n",
    "        else:\n",
    "            temp_seq_str = temp_seq_str + fasta_in[i]\n",
    "    #finally we need to add in the last sequence\n",
    "    fasta_out.append(temp_seq_str)\n",
    "    return fasta_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the predicted orfs\n",
    "# read in the orf aa files for each of the four species\n",
    "aa_orf_file_holder_list = []\n",
    "for spp in list(gene_id_df):\n",
    "    with open('/home/baumgas/projects/done/7_John/assemblies_species/annotation/for-dnds/{}_ORFaa.fa'.format(spp),\n",
    "              'r') as f:\n",
    "        aa_orf_file_holder_list.append(convert_interleaved_to_sequencial_fasta_two([line.rstrip() for line in f]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dict that relates compIDs with multiple ORFs predicted to a list of the ORF IDs predicted for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_to_orfs_dict_holder_list = [defaultdict(list) for spp in list(gene_id_df)]\n",
    "orf_to_aa_dict_holder_list = [dict() for spp in list(gene_id_df)]\n",
    "for i in range(len(list(gene_id_df))):\n",
    "    for j in range(len(aa_orf_file_holder_list[i])):\n",
    "        if aa_orf_file_holder_list[i][j].startswith('>'):\n",
    "            comp_ID = aa_orf_file_holder_list[i][j].split()[8].split('_')[0]\n",
    "            orf_ID = aa_orf_file_holder_list[i][j].split()[0][1:]\n",
    "            comp_to_orfs_dict_holder_list[i][comp_ID].append(orf_ID)\n",
    "            orf_to_aa_dict_holder_list[i][orf_ID] = aa_orf_file_holder_list[i][j + 1]\n",
    "    # print out stats for the default dict to see if we agree with Chris at this point\n",
    "    multi_orf_comps = sum([1 for k, v in comp_to_orfs_dict_holder_list[i].items() if len(v) > 1])\n",
    "    print('{} comps with > 1 ORF in {}'.format(multi_orf_comps, list(gene_id_df)[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now make a list from this which is simply the comps that have multiple ORFs predicted for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we have the list of dictionaries from which we can get the comp IDs that have multiple ORFs\n",
    "# perhaps we shoud convert them to lists now rather than on the fly\n",
    "list_of_multi_orf_comps_per_spp = [[k for k, v in comp_to_orfs_dict_holder_list[i].items() if len(v) > 1] for i in range(len(list(gene_id_df)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go through the df of compIDs for each ortholog and for each species look to see if the comp ID is found in the list we just created\n",
    "If it is then this means that there at least one of the speceis has a transcript for which multiple ORFs were predicted. The set of ORFs associated with this ortholog will therefore need to be investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will multiprocess the checks of the problematic orthologs.\n",
    "To do this we will create a list that will hold three items:\n",
    "* A list of tuples. Each tuple containing four aa sequences. The collection of tuples represent all of the possible alignments that we need to check\n",
    "* The list of aa sequenecs that are currently assigned to the ortholog\n",
    "* The ortholog ID (simply an int)\n",
    "\n",
    "#### create the list that will hold the MP info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_labels = list(gene_id_df)\n",
    "for index in gene_id_df.index.values.tolist():\n",
    "    # within each row check to see if the comp is in its respective list\n",
    "    row_to_check = False\n",
    "    for i in range(len(col_labels)):\n",
    "        if gene_id_df.loc[index, col_labels[i]] in list_of_multi_orf_comps_per_spp[i]:\n",
    "            row_to_check = True\n",
    "    # here we have checked through each of the comps in the row\n",
    "    # if row_to_check == True then we need to work through each of the comps and see which ORF combinations\n",
    "    # produce the best average pairwise distances\n",
    "\n",
    "    if row_to_check:\n",
    "        print('Checking multi-ORF ortholog {}'.format(index), end='\\r')\n",
    "        multi_orf_counter += 1\n",
    "        list_of_lists_of_possible_orfs = [comp_to_orfs_dict_holder_list[i][gene_id_df.loc[index, col_labels[i]]] for i in range(len(col_labels))]\n",
    "\n",
    "\n",
    "        # now run itertools.product on this list of lists to get the tuples which are essentially\n",
    "        # the different orfs that we would be trying to align\n",
    "        list_of_lists_of_possible_orfs_as_aa = [[] for spp in col_labels]\n",
    "        for k in range(len(list_of_lists_of_possible_orfs)):\n",
    "            for orfID in list_of_lists_of_possible_orfs[k]:\n",
    "                list_of_lists_of_possible_orfs_as_aa[k].append(orf_to_aa_dict_holder_list[k][orfID])\n",
    "        # hardcode it to a list so that we can get the index of each tuple below\n",
    "        alignment_tuples = [tup for tup in itertools.product(*list_of_lists_of_possible_orfs_as_aa)]\n",
    "\n",
    "\n",
    "        mp_list.append((alignment_tuples, aa_seq_df.loc[index].tolist(), index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the MP worker that will perform the pairwise comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ORF_screening_worker(input_queue, rows_to_be_replaced_dict):\n",
    "    for tup in iter(input_queue.get, 'STOP'):\n",
    "        alignment_tuples, current_seqs, index = tup\n",
    "        print('Processing multi-ORF ortholog: {}'.format(index))\n",
    "        # we should now have sets that are four aa sequences.\n",
    "        # for each set, generate pairwise comparisons and calculate pairwise distances\n",
    "        # keep track of each distance and calculate average PW distances for each set_of_alignemtn_seqs\n",
    "        average_distance_list = []\n",
    "        for set_of_alignment_seqs in alignment_tuples:\n",
    "            temp_pairwise_scores_list = []\n",
    "            for seq_a, seq_b in itertools.combinations(set_of_alignment_seqs, 2):\n",
    "                # here is a single PW distance calculation\n",
    "                score = pairwise2.align.globalxx(seq_a, seq_b, score_only=True)\n",
    "                temp_pairwise_scores_list.append(score)\n",
    "            # now calculate average\n",
    "            temp_average = sum(temp_pairwise_scores_list) / len(temp_pairwise_scores_list)\n",
    "            average_distance_list.append(temp_average)\n",
    "\n",
    "        # now we have a list of PW distance for each of the sets of sequences (virtual alignments)\n",
    "        # we want to select the set that has the highest score\n",
    "\n",
    "        index_of_best_set = average_distance_list.index(max(average_distance_list))\n",
    "        # now check to see if these match those that were already chosen\n",
    "        best_set_of_aas = alignment_tuples[index_of_best_set]\n",
    "        alignments_are_same = True\n",
    "\n",
    "        for i in range(len(current_seqs)):\n",
    "            if current_seqs[i] != best_set_of_aas[i]:\n",
    "                alignments_are_same = False\n",
    "                break\n",
    "\n",
    "        if not alignments_are_same:\n",
    "            # print('Fasta for ortholog: {}'.format(index))\n",
    "            # for n in range(len(best_set_of_aas)):\n",
    "            #     print('>{}\\n{}'.format(n, alignment_tuples[index_of_best_set][n]))\n",
    "            # we don't need to know the actual ORFs that have been chosen for each spp\n",
    "            # only the aa codes so let's just output this result\n",
    "            rows_to_be_replaced_dict[index] = [aa for aa in alignment_tuples[index_of_best_set]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and run the MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = 20\n",
    "\n",
    "#Queue that will hold the index of the rows that need to be checked\n",
    "input_queue = Queue()\n",
    "\n",
    "# populate input_queue\n",
    "for tup in mp_list:\n",
    "    input_queue.put(tup)\n",
    "\n",
    "for i in range(num_proc):\n",
    "    input_queue.put('STOP')\n",
    "\n",
    "# Manager for a dict rows_to_be_replaced_dict that will hold the new aa_seqs for the fixed indices\n",
    "manager = Manager()\n",
    "rows_to_be_replaced_dict = manager.dict()\n",
    "\n",
    "list_of_processes = []\n",
    "for N in range(num_proc):\n",
    "    p = Process(target=ORF_screening_worker, args=(input_queue, rows_to_be_replaced_dict))\n",
    "    list_of_processes.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in list_of_processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have a dictionary that contains the ID of an ortholog as key and a list of the four aa seqs (one for each sequence) that need to be __replaced in the dataframe and then written out__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_counter = len(rows_to_be_replaced_dict.keys())\n",
    "# now replace the dataframe values\n",
    "for index in aa_seq_df.index.values.tolist():\n",
    "    if index in rows_to_be_replaced_dict.keys():\n",
    "        # then this is a row that needs replcing with the new values\n",
    "        aa_seq_df.loc[index] = rows_to_be_replaced_dict[index]\n",
    "\n",
    "\n",
    "# at this point it only remains to write the data frame out as csv\n",
    "print('{} orthologs were checked due to multi-ORFs\\n'\n",
    "      '{} were fixed\\n'\n",
    "      '{} already contained the optimal choice of ORFs\\n'.format\n",
    "    (\n",
    "    multi_orf_counter, fixed_counter, multi_orf_counter-fixed_counter\n",
    "    )\n",
    ")\n",
    "aa_seq_df.to_csv('/home/humebc/projects/parky/aa_seq_multi_orf_orths_fixed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the super alignment and phylogeny\n",
    "* Generate local alignment for each ortholog (MAFFT; cropped)\n",
    "* Calculate best fit amino acid substitution matrix (using prottest)\n",
    "* Concatenate the local alignements into super alignment, create q file and make ML tree (using raxml_HPC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate local alignment for each ortholog (MAFFT; cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Local functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDefinedFileToList(filename):\n",
    "    temp_list = []\n",
    "    with open(filename, mode='r') as reader:\n",
    "        temp_list = [line.rstrip() for line in reader]\n",
    "    return temp_list\n",
    "\n",
    "def writeListToDestination(destination, listToWrite):\n",
    "    #print('Writing list to ' + destination)\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(destination))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    with open(destination, mode='w') as writer:\n",
    "        i = 0\n",
    "        while i < len(listToWrite):\n",
    "            if i != len(listToWrite)-1:\n",
    "                writer.write(listToWrite[i] + '\\n')\n",
    "            elif i == len(listToWrite)-1:\n",
    "                writer.write(listToWrite[i])\n",
    "            i += 1\n",
    "\n",
    "def convert_interleaved_to_sequencial_fasta_two(fasta_in):\n",
    "    fasta_out = []\n",
    "\n",
    "    for i in range(len(fasta_in)):\n",
    "\n",
    "        if fasta_in[i].startswith('>'):\n",
    "            if fasta_out:\n",
    "                # if the fasta is not empty then this is not the first\n",
    "                fasta_out.append(temp_seq_str)\n",
    "            #else then this is the first sequence and there is no need to add the seq.\n",
    "            temp_seq_str = ''\n",
    "            fasta_out.append(fasta_in[i])\n",
    "        else:\n",
    "            temp_seq_str = temp_seq_str + fasta_in[i]\n",
    "    #finally we need to add in the last sequence\n",
    "    fasta_out.append(temp_seq_str)\n",
    "    return fasta_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the aa and gene ID csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the amino acid sequences\n",
    "aa_seq_array = pd.read_csv('/home/humebc/projects/parky/aa_seq_multi_orf_orths_fixed.csv', sep=',', lineterminator='\\n', index_col=0, header=0)\n",
    "\n",
    "# the gene IDs\n",
    "gene_id_array = pd.read_csv('/home/humebc/projects/parky/gene_id_fixed.csv', sep=',', lineterminator='\\n', index_col=0, header=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the alignments using multiprocessing to speed things up.\n",
    "This worker will perform the alignment and also do the cropping. We will do the cropping by reading in an alignment into a dataframe and then drop the columns from the dataframe that contained gaps from both the beggining and the end, until we come to a column that doesn't contain a gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_local_alignment_worker(input, output_dir, spp_list):\n",
    "    # for each list that represents an ortholog\n",
    "    for k_v_pair in iter(input.get, 'STOP'):\n",
    "\n",
    "        # ortholog_id\n",
    "        ortholog_id = k_v_pair[0]\n",
    "        print('Processing {}'.format(ortholog_id))\n",
    "        # ortholog_spp_seq_list\n",
    "        ortholog_spp_seq_list = k_v_pair[1]\n",
    "\n",
    "        # create the fasta\n",
    "        fasta_file = []\n",
    "\n",
    "        # for each species\n",
    "        # add the name and aa_seq to the fasta_file\n",
    "        for i in range(len(spp_list)):\n",
    "            fasta_file.extend(['>{}_{}'.format(spp_list[i], ortholog_spp_seq_list[i][0]), ortholog_spp_seq_list[i][1]])\n",
    "\n",
    "        # here we have the fasta_file populated\n",
    "\n",
    "        # Write out the new fasta\n",
    "        fasta_output_path = '{}/{}.fasta'.format(output_dir, ortholog_id)\n",
    "        writeListToDestination(fasta_output_path, fasta_file)\n",
    "        # now perform the alignment with MAFFT\n",
    "        mafft = local[\"mafft-linsi\"]\n",
    "        in_file = fasta_output_path\n",
    "        out_file = fasta_output_path.replace('.fasta', '_aligned.fasta')\n",
    "        # now run mafft including the redirect\n",
    "        (mafft[in_file] > out_file)()\n",
    "\n",
    "        # at this point we have the aligned .fasta written to the output directory\n",
    "        # at this point we need to trim the fasta.\n",
    "        # I was going to use trimAl but this doesn't actually have an option to clean up the ends of alignments\n",
    "        # instead, read in the alignment as a TwoD list to a pandas dataframe\n",
    "        # then delete the begining and end columns that contain gap sites\n",
    "        aligned_fasta_interleaved = readDefinedFileToList(out_file)\n",
    "        aligned_fasta = convert_interleaved_to_sequencial_fasta_two(aligned_fasta_interleaved)\n",
    "        array_list = []\n",
    "        for i in range(1, len(aligned_fasta), 2):\n",
    "                array_list.append(list(aligned_fasta[i]))\n",
    "\n",
    "        # make into pandas dataframe\n",
    "        alignment_df = pd.DataFrame(array_list)\n",
    "\n",
    "        # go from either end deleting any columns that have a gap\n",
    "        columns_to_drop = []\n",
    "        for i in list(alignment_df):\n",
    "            # if there is a gap in the column at the beginning\n",
    "            if '-' in list(alignment_df[i]) or '*' in list(alignment_df[i]):\n",
    "                columns_to_drop.append(i)\n",
    "            else:\n",
    "                break\n",
    "        for i in reversed(list(alignment_df)):\n",
    "            # if there is a gap in the column at the end\n",
    "            if '-' in list(alignment_df[i]) or '*' in list(alignment_df[i]):\n",
    "                columns_to_drop.append(i)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # get a list that is the columns indices that we want to keep\n",
    "        col_to_keep = [col_index for col_index in list(alignment_df) if col_index not in columns_to_drop]\n",
    "\n",
    "        # drop the gap columns\n",
    "        alignment_df = alignment_df[col_to_keep]\n",
    "\n",
    "        # here we have the pandas dataframe with the gap columns removed\n",
    "        #convert back to a fasta and write out\n",
    "        cropped_fasta = []\n",
    "        alignment_index_labels = list(alignment_df.index)\n",
    "        for i in range(len(alignment_index_labels)):\n",
    "            seq_name = '>{}_{}'.format(spp_list[i], ortholog_spp_seq_list[i][0])\n",
    "            aa_seq = ''.join(alignment_df.loc[alignment_index_labels[i]])\n",
    "            cropped_fasta.extend([seq_name, aa_seq])\n",
    "\n",
    "        # here we have the cropped and aligned fasta\n",
    "        # write it out\n",
    "        aligned_cropped_fasta_path = fasta_output_path.replace('.fasta', '_aligned_cropped.fasta')\n",
    "        writeListToDestination(aligned_cropped_fasta_path, cropped_fasta)\n",
    "\n",
    "        # here we should be done with the single alignment\n",
    "        print('Local alignment for {} completed'.format(ortholog_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and populate the input Queue for the MP process. Then run.\n",
    "The items in this list will be key value pairs from the dictionary that we will create below. The dictionary will simply be ortholog ID = key and list of aa seqs for that ortholog will be value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making MP data_holder_list\n",
    "tuple_holder_dict = {}\n",
    "#for each ortholog\n",
    "for row_index in aa_seq_array.index.values.tolist():\n",
    "    print('Adding ortholog {} to MP info list'.format(row_index))\n",
    "    ortholog_id_seq_list = []\n",
    "    # for each species.\n",
    "    for spp in list(gene_id_array):\n",
    "        gene_id = gene_id_array[spp][row_index]\n",
    "        aa_seq = aa_seq_array[spp][row_index]\n",
    "        ortholog_id_seq_list.append((gene_id, aa_seq))\n",
    "    tuple_holder_dict[row_index] = ortholog_id_seq_list\n",
    "\n",
    "# creating the MP input queue\n",
    "ortholog_input_queue = Queue()\n",
    "\n",
    "# populate with one key value pair per ortholog\n",
    "for key, value in tuple_holder_dict.items():\n",
    "    print('Placing {} in MP queue'.format(key))\n",
    "    ortholog_input_queue.put((key, value))\n",
    "\n",
    "num_proc = 24\n",
    "\n",
    "# put in the STOPs\n",
    "for N in range(num_proc):\n",
    "    ortholog_input_queue.put('STOP')\n",
    "\n",
    "allProcesses = []\n",
    "\n",
    "# directory to put the local alignments\n",
    "output_dir = '/home/humebc/projects/parky/aa_tree_creation/local_alignments'\n",
    "\n",
    "# the list of species for each ortholog\n",
    "spp_list = [spp for spp in list(gene_id_array)]\n",
    "\n",
    "# Then start the workers\n",
    "for N in range(num_proc):\n",
    "    p = Process(target=create_local_alignment_worker, args=(ortholog_input_queue, output_dir, spp_list))\n",
    "    allProcesses.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in allProcesses:\n",
    "    p.join()\n",
    "\n",
    "# at this point we have the local alignments all written as fasta files to output_dir.\n",
    "# Now it just remains to concatenate and then run the ML tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "### Calculate best fit amino acid substitution matrix (using prottest)\n",
    "We will use prottest to calculate the model for each of the local alignments. We will MP this. This part of the program takes a considerable amount of time and I ran it over night to complete. You can run it using tmux. Once the models had been output I checked to see that none of the outputs had been corupted (and thus would cause problems further down the line) by simply checking the size of the output files. Most files were 33k in size. Files smaller than this were checked (about 20). These all had IO errors and had stopped prematurely. So, I ran the code again (it has a built in check to see if the output has already been produced; in which case it will not re-do). This time the check came up with no files less than 33k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prottest_worker(input_queue):\n",
    "    base_dir = '/home/humebc/projects/parky/aa_tree_creation/local_alignments'\n",
    "    for file_name in iter(input_queue.get, 'STOP'):\n",
    "        input_path = '{}/{}'.format(base_dir, file_name)\n",
    "        output_path = input_path.replace('_aligned_cropped.fasta', '_prottest_result.out')\n",
    "        if os.path.isfile(output_path):\n",
    "            continue\n",
    "        sys.stdout.write('\\rRunning prottest: {}'.format(file_name))\n",
    "        # perform the prottest\n",
    "        prot_path = '/home/humebc/phylogeneticsSoftware/protest/prottest-3.4.2/prottest-3.4.2.jar'\n",
    "        subprocess.run(['java', '-jar', prot_path, '-i', input_path, '-o', output_path, '-all-distributions', '-all']\n",
    "                       , stdout=subprocess.PIPE, stderr=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will find each of the local alignments and run put them into a list which we will MP\n",
    "# for each item we will run prottest with a single thread and output a file\n",
    "# in the concatenate local alignments file we will then create a q file that will\n",
    "# designate the different partitions and which of the substitution models to use.\n",
    "\n",
    "# get a list of all of the fasta names that we will want to concatenate\n",
    "base_dir = '/home/humebc/projects/parky/aa_tree_creation/local_alignments'\n",
    "list_of_files = [f for f in os.listdir(base_dir) if 'aligned_cropped.fasta' in f]\n",
    "\n",
    "\n",
    "num_proc = 12\n",
    "\n",
    "# Queue that will hold the index of the rows that need to be checked\n",
    "input_queue = Queue()\n",
    "\n",
    "# populate input_queue\n",
    "for file_name in list_of_files:\n",
    "    input_queue.put(file_name)\n",
    "\n",
    "for i in range(num_proc):\n",
    "    input_queue.put('STOP')\n",
    "\n",
    "list_of_processes = []\n",
    "for N in range(num_proc):\n",
    "    p = Process(target=prottest_worker, args=(input_queue,))\n",
    "    list_of_processes.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in list_of_processes:\n",
    "    p.join()\n",
    "\n",
    "return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "### Concatenate the local alignements into super alignment, create q file and make ML tree (using raxml_HPC)\n",
    "To do this we read in all of the prot model output files and look to see which model was suggested for each of the local alignments. We make a default dictionary that holds key = model, value = list(the ortholog IDs that use that model). We then go model by model from this dict and within that ortholog ID by ortholog ID to concatenate all of the local alignments together. At the same time that we are doing the concatenation of the local alignments we are also producing the q_file on a model by model basis. The q file is a file that tells raxml how to partition the data and in our case which protein model should be used in conjunction with which partition. By grouping together the local alignments by the model they will use and therefore minimising the partitions we will hopefully be gaining significant advantages in comput time in the raxml stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dictionary of model to ortholog IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The master alignment that we create should be partitioned according to the protein model used.\n",
    "# I have generated all of the .out files which are the outputs from the prottest\n",
    "# We should iter through these and create a dictionary that is a model type as key\n",
    "# and then have a list of the orthologs of that model.\n",
    "# then sort this by the length of the list\n",
    "# then work our way through the local alignments in this order creating the alignment\n",
    "# We will need to generate the p file as we go\n",
    "# this should take the form\n",
    "'''\n",
    "JTT, gene1 = 1-500\n",
    "WAGF, gene2 = 501-800\n",
    "WAG, gene3 = 801-1000\n",
    "\n",
    "'''\n",
    "\n",
    "# get list of the .out prottest files\n",
    "base_dir = '/home/humebc/projects/parky/aa_tree_creation/local_alignments'\n",
    "list_of_prot_out_filenames = [f for f in os.listdir(base_dir) if 'prottest' in f]\n",
    "\n",
    "# iter through the list of protfiles creating the dict relating model to ortholog\n",
    "# we cannnot change the +G or +I for each partition. As such I will define according to the base model\n",
    "model_to_orth_dict = defaultdict(list)\n",
    "for i in range(len(list_of_prot_out_filenames)):\n",
    "    model = ''\n",
    "    file_name = list_of_prot_out_filenames[i]\n",
    "    orth_num = int(file_name.split('_')[0])\n",
    "    with open('{}/{}'.format(base_dir, file_name), 'r') as f:\n",
    "        temp_file_list = [line.rstrip() for line in f]\n",
    "    for j in range(300, len(temp_file_list), 1):\n",
    "        if 'Best model according to BIC' in temp_file_list[j]:\n",
    "            model = temp_file_list[j].split(':')[1].strip().replace('+G','').replace('+I','')\n",
    "            break\n",
    "    if model == '':\n",
    "        sys.exit('Model line not found in {}'.format(orth_num))\n",
    "    model_to_orth_dict[model].append(orth_num)\n",
    "\n",
    "# #N.B. that we cannot have different gamma for different partitions\n",
    "# # Also best advice is not to run +G and +I together.\n",
    "# # As such we only need to extract the base model here i.e. WAG rather than WAG [+G|+I]\n",
    "# for model in model_to_orth_dict\n",
    "\n",
    "print('The 19k sequences are best represented by {} different aa models'.format(len(model_to_orth_dict.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go model by model concatenating and making the q file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we have the dict populated\n",
    "# now sort the dict\n",
    "sorted_model_list = sorted(model_to_orth_dict, key=lambda k: len(model_to_orth_dict[k]), reverse=True)\n",
    "\n",
    "# now go model by model in the sorted_model_list to make the master alignment.\n",
    "\n",
    "# not the most elegant way but I think I'll just create the mast fasta in memory\n",
    "master_fasta = ['>min','', '>pmin', '', '>psyg', '', '>ppsyg', '']\n",
    "\n",
    "# The q file will hold the information for the partitioning of the alignment for the raxml analysis\n",
    "q_file = []\n",
    "for model in sorted_model_list:\n",
    "    q_file_start = len(master_fasta[1]) + 1\n",
    "    sys.stdout.write('\\rProcessing model {} sequences'.format(model))\n",
    "    for orth_num in model_to_orth_dict[model]:\n",
    "        file_name = str(orth_num) + '_aligned_cropped.fasta'\n",
    "        with open('{}/{}'.format(base_dir, file_name), 'r') as f:\n",
    "            temp_list_of_lines = [line.rstrip() for line in f]\n",
    "\n",
    "        for i in range(1, len(temp_list_of_lines), 2):\n",
    "            new_seq = master_fasta[i] + temp_list_of_lines[i]\n",
    "            master_fasta[i] = new_seq\n",
    "    q_file_finish = len(master_fasta[1])\n",
    "    q_file.append('{}, gene{} = {}-{}'.format(\n",
    "        model.upper(), sorted_model_list.index(model) + 1, q_file_start, q_file_finish))\n",
    "\n",
    "# here we have the master fasta and the q file ready to be outputted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write out the master fasta and the q file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now write out the master fasta\n",
    "master_fasta_output_path = '/home/humebc/projects/parky/aa_tree_creation/master.fasta'\n",
    "with open(master_fasta_output_path, 'w') as f:\n",
    "    for line in master_fasta:\n",
    "        f.write('{}\\n'.format(line))\n",
    "\n",
    "# now write out the q file\n",
    "q_file_output_path = '/home/humebc/projects/parky/aa_tree_creation/qfile.q'\n",
    "with open(q_file_output_path, 'w') as f:\n",
    "    for line in q_file:\n",
    "        f.write('{}\\n'.format(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it just remains to run the raxml.\n",
    "There are a lot of options here to get it to run right\n",
    "Breifly (to helpfully save time next time)\n",
    "* -s = input file\n",
    "* -q = the q file that defines the partitions\n",
    "* -x = this switches on rapid bootstrapping and provides a random number to initiate\n",
    "* -f a = Thi means that the summarised tree will also have the bootstrapped values put on it in the output\n",
    "* -p = a seed required by raxml (random number)\n",
    "* -# the number of bootstraps\n",
    "* -n the base of the output files\n",
    "* -w the output directory where the files will be written\n",
    "* -T the threads used (processes)\n",
    "* -m the model used (see comment in code)\n",
    "\n",
    "NB I tried to get the AVX2 version of raxml to work but the architechture of Symbiomics did not support it so we are running the AVX. Also please see the comment in the code on the -m flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run raxml\n",
    "#NB note that although we are specificing mdels for each partition, we still need to add the PROTGAMMAIWAG\n",
    "# model argument to the -m flag. This is just a weird operation of raxml and is explained but hidden in the manual\n",
    "# (search for 'If you want to do a partitioned analysis of concatenated'). Raxml will only extract the rate\n",
    "# variation information from this and will ignore the model component e.g. WAG. FYI any model could be used\n",
    "# doesn't have to be WAG.\n",
    "raxml_path = '/home/humebc/phylogeneticsSoftware/raxml/standard-RAxML/raxmlHPC-PTHREADS-AVX'\n",
    "subprocess.run([raxml_path, '-s', master_fasta_output_path, '-q', q_file_output_path,\n",
    "                '-x', '183746', '-f', 'a', '-p', '83746273', '-#', '100', '-T', '8', '-n', 'parkinson_out',\n",
    "                '-m', 'PROTGAMMAWAG', '-w', '/home/humebc/projects/parky/aa_tree_creation'])\n",
    "\n",
    "print('\\nConstruction of master fasta complete:\\n{}\\n{}'.format(master_fasta_output_path, q_file_output_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
